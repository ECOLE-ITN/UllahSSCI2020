{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyDOE\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bbobbenchmarks as bn\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "from collections import namedtuple\n",
    "import cma\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.stats.distributions as dist\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sklearn\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New Reduced Data Sets for all Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_f2(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_2_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f3(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_3_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f7(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_7_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f9(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_9_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f10(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_10_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f13(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_13_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f15(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_15_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f16(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_16_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f20(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_20_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f24(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_24_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Models & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueRange = namedtuple('ValueRange', ['min', 'max'])\n",
    "\n",
    "def determinerange(values):\n",
    "    \"\"\"Determine the range of values in each dimension\"\"\"\n",
    "    return ValueRange(np.min(values, axis=0), np.max(values, axis=0))\n",
    "\n",
    "\n",
    "def linearscaletransform(values, *, range_in=None, range_out=ValueRange(0, 1), scale_only=False):\n",
    "    \"\"\"Perform a scale transformation of `values`: [range_in] --> [range_out]\"\"\"\n",
    "\n",
    "    if range_in is None:\n",
    "        range_in = determinerange(values)\n",
    "    elif not isinstance(range_in, ValueRange):\n",
    "        range_in = ValueRange(*range_in)\n",
    "\n",
    "    if not isinstance(range_out, ValueRange):\n",
    "        range_out = ValueRange(*range_out)\n",
    "\n",
    "    scale_out = range_out.max - range_out.min\n",
    "    scale_in = range_in.max - range_in.min\n",
    "\n",
    "    if scale_only:\n",
    "        scaled_values = (values / scale_in) * scale_out\n",
    "    else:\n",
    "        scaled_values = (values - range_in.min) / scale_in\n",
    "        scaled_values = (scaled_values * scale_out) + range_out.min\n",
    "\n",
    "    return scaled_values\n",
    "\n",
    "''' F2 '''\n",
    "def F2(X):\n",
    "    f = bn.F2()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F3 '''\n",
    "def F3(X):\n",
    "    f = bn.F3()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F7 '''\n",
    "def F7(X):\n",
    "    f = bn.F7()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F9 '''\n",
    "def F9(X):\n",
    "    f = bn.F9()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F10 '''\n",
    "def F10(X):\n",
    "    f = bn.F10()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F13 '''\n",
    "def F13(X):\n",
    "    f = bn.F13()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F15 '''\n",
    "def F15(X):\n",
    "    f = bn.F15()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F16 '''\n",
    "def F16(X):\n",
    "    f = bn.F16()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F20 '''\n",
    "def F20(X):\n",
    "    f = bn.F20()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F24 '''\n",
    "def F24(X):\n",
    "    f = bn.F24()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' Latin HyperCube Sampling Design of Experiment '''\n",
    "def DOE(n_obs, dim):\n",
    "    np.random.seed(0)\n",
    "    lhd = pyDOE.lhs(n=dim, samples=n_obs, criterion='m')\n",
    "    X = [lhd[:,idx] for idx in range(dim)]\n",
    "    return X\n",
    "\n",
    "''' Kriging'''\n",
    "def kriging(train_data,test_data):\n",
    "    kernel =  RationalQuadratic()\n",
    "    scaler = MinMaxScaler().fit(np.r_[train_data.iloc[:,:-1].values])\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel,n_restarts_optimizer= 15,random_state=0,\n",
    "                                   normalize_y=True ).fit(scaler.transform(train_data.iloc[:,:-1]), train_data.iloc[:,-1])\n",
    "    pred = gpr.predict(scaler.transform(test_data))\n",
    "    def predict(scaler, gpr):\n",
    "        def __predict__(x):\n",
    "            x = np.atleast_2d(x)\n",
    "            return gpr.predict(scaler.transform(x))\n",
    "        return __predict__\n",
    "    return gpr,pred, predict(scaler,gpr)\n",
    "\n",
    "\"\"\" This method implements and evaluates the Kriging Surrogate Model with RMAE \"\"\"\n",
    "def surrogate_model(train_data,test_data):\n",
    "    model_kri , pred_kri , predict_kri = kriging(train_data,test_data.iloc[:,:-1])\n",
    "    return predict_kri\n",
    "\n",
    "\"\"\" Implements all the surrogate models, i.e., for all test function, and returns the median of RMAE errors,\n",
    "This median is used as the primary metric for Hyper-Parameters Optimization \"\"\"\n",
    "def perform_surrogate_modeling(paths, path_latent_train,path_latent_test):\n",
    "    train_2, test_2, _ = load_f2(paths[0],path_latent_train,path_latent_test)\n",
    "    predict_kri_2 = surrogate_model(train_2, test_2)\n",
    "    \n",
    "    train_3, test_3, _ = load_f3(paths[1],path_latent_train,path_latent_test)\n",
    "    predict_kri_3 = surrogate_model(train_3, test_3)\n",
    "    \n",
    "    train_7, test_7, _ = load_f7(paths[2],path_latent_train,path_latent_test)\n",
    "    predict_kri_7 = surrogate_model(train_7, test_7)\n",
    "    \n",
    "    train_9, test_9, _ = load_f9(paths[3],path_latent_train,path_latent_test)\n",
    "    predict_kri_9 = surrogate_model(train_9, test_9)\n",
    "    \n",
    "    train_10, test_10, _ = load_f10(paths[4],path_latent_train,path_latent_test)\n",
    "    predict_kri_10 = surrogate_model(train_10, test_10)\n",
    "    \n",
    "    train_13, test_13, _ = load_f13(paths[5],path_latent_train,path_latent_test)\n",
    "    predict_kri_13 = surrogate_model(train_13, test_13)\n",
    "    \n",
    "    train_15, test_15, _ = load_f15(paths[6],path_latent_train,path_latent_test)\n",
    "    predict_kri_15 = surrogate_model(train_15, test_15)\n",
    "    \n",
    "    train_16, test_16, _ = load_f16(paths[7],path_latent_train,path_latent_test)\n",
    "    predict_kri_16 = surrogate_model(train_16, test_16)\n",
    "    \n",
    "    train_20, test_20, _ = load_f20(paths[8],path_latent_train,path_latent_test)\n",
    "    predict_kri_20 = surrogate_model(train_20, test_20)\n",
    "    \n",
    "    train_24, test_24, _ = load_f24(paths[9],path_latent_train,path_latent_test)\n",
    "    predict_kri_24 = surrogate_model(train_24, test_24)\n",
    "    \n",
    "    kri = [predict_kri_2,predict_kri_3,predict_kri_7,predict_kri_9,predict_kri_10,predict_kri_13,\n",
    "          predict_kri_15,predict_kri_16,predict_kri_20,predict_kri_24]\n",
    "    \n",
    "    return kri\n",
    "\"\"\" This is the function used for Hyper_Parameters_Optimization for both dimensionality reduction and surrogate modelling \"\"\"\n",
    "def run(path_latent_train,path_latent_test):\n",
    "    kri = perform_surrogate_modeling (paths,path_latent_train,path_latent_test)\n",
    "    return kri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "path_2 = \"Data Generation/50 D/Training_Data_Sets/train_2_1000Samples.csv\"\n",
    "path_3 = \"Data Generation/50 D/Training_Data_Sets/train_3_1000Samples.csv\"\n",
    "path_7 = \"Data Generation/50 D/Training_Data_Sets/train_7_1000Samples.csv\"\n",
    "path_9 = \"Data Generation/50 D/Training_Data_Sets/train_9_1000Samples.csv\"\n",
    "path_10 = \"Data Generation/50 D/Training_Data_Sets/train_10_1000Samples.csv\"\n",
    "path_13 = \"/Data Generation/50 D/Training_Data_Sets/train_13_1000Samples.csv\"\n",
    "path_15 = \"Data Generation/50 D/Training_Data_Sets/train_15_1000Samples.csv\"\n",
    "path_16 = \"Data Generation/50 D/Training_Data_Sets/train_16_1000Samples.csv\"\n",
    "path_20 = \"Data Generation/50 D/Training_Data_Sets/train_201000Samples.csv\"\n",
    "path_24 = \"Data Generation/50 D/Training_Data_Sets/train_241000Samples.csv\"\n",
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \n",
    "path_latent_train = \"VAEs_Kriging_50D_30%_latent_training.csv\"\n",
    "path_latent_test = \"VAEs_Kriging_50D_30%_latent_test.csv\"\n",
    "paths = [path_2,path_3,path_7,path_9,path_10,path_13,path_15,path_16,path_20,path_24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Dimensionality Reduction and Construct the Surrogate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kri = run(path_latent_train,path_latent_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = int(50 - (0.3 * 50)) # change here\n",
    "Columns = ['Kri']\n",
    "Cols = []\n",
    "for j in range(len(Columns)):\n",
    "    for i in range(1,dimensionality+1):\n",
    "        Cols.append(Columns[j]+'_Z'+str(i))\n",
    "opt = {'maxfun':1000 * dimensionality,'ftol':0.0}\n",
    "const = Bounds([-5] * dimensionality, [5] * dimensionality)        \n",
    "n_obs =  30\n",
    "G = DOE(n_obs, dimensionality)\n",
    "G = [ G[idx].reshape(n_obs,1) for idx in range(len(G)) ]\n",
    "X_2_Values = np.zeros([n_obs,dimensionality])\n",
    "X_3_Values = np.zeros([n_obs,dimensionality])\n",
    "X_7_Values = np.zeros([n_obs,dimensionality])\n",
    "X_9_Values = np.zeros([n_obs,dimensionality])\n",
    "X_10_Values = np.zeros([n_obs,dimensionality])\n",
    "X_13_Values = np.zeros([n_obs,dimensionality])\n",
    "X_15_Values = np.zeros([n_obs,dimensionality])\n",
    "X_16_Values = np.zeros([n_obs,dimensionality])\n",
    "X_20_Values = np.zeros([n_obs,dimensionality])\n",
    "X_24_Values = np.zeros([n_obs,dimensionality])\n",
    "\n",
    "for i in range(G[1].shape[0]):\n",
    "    min_kri_2  = minimize(fun=kri[0], x0 = linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_3  = minimize(fun=kri[1],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_7  = minimize(fun=kri[2],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_9  = minimize(fun=kri[3],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_10  = minimize(fun=kri[4],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_13  = minimize(fun=kri[5],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_15  = minimize(fun=kri[6],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_16  = minimize(fun=kri[7],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_20  = minimize(fun=kri[8],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_kri_24  = minimize(fun=kri[9],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "\n",
    "    X_2_Values [i,:] = list(min_kri_2.x)\n",
    "    X_3_Values [i,:] = list(min_kri_3.x)\n",
    "    X_7_Values [i,:] = list(min_kri_7.x)\n",
    "    X_9_Values [i,:] = list(min_kri_9.x)\n",
    "    X_10_Values [i,:] = list(min_kri_10.x)                                           \n",
    "    X_13_Values [i,:] = list(min_kri_13.x)\n",
    "    X_15_Values [i,:] = list(min_kri_15.x)\n",
    "    X_16_Values [i,:] = list(min_kri_16.x) \n",
    "    X_20_Values [i,:] = list(min_kri_20.x) \n",
    "    X_24_Values [i,:] = list(min_kri_24.x)                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kriging-f2\n",
      "12749360.253873594\n",
      "Kriging-f3\n",
      "459.2515528754444\n",
      "Kriging-f7\n",
      "246.66535824549322\n",
      "Kriging-f9\n",
      "376755.0439359674\n",
      "Kriging-f10\n",
      "2092110.9098707326\n",
      "Kriging-f13\n",
      "664.8472014915444\n",
      "Kriging-f15\n",
      "494.06884907067547\n",
      "Kriging-f16\n",
      "-187.97794034154498\n",
      "Kriging-f20\n",
      "72943.92056008095\n",
      "Kriging-f24\n",
      "239.85594513025416\n"
     ]
    }
   ],
   "source": [
    "X_2_Values = pd.DataFrame(X_2_Values)\n",
    "X_2_Values.columns = Cols\n",
    "X_2_Values.to_csv('F2_X_Values.csv')\n",
    "\n",
    "X_3_Values = pd.DataFrame(X_3_Values)\n",
    "X_3_Values.columns = Cols\n",
    "X_3_Values.to_csv('F3_X_Values.csv')\n",
    "\n",
    "X_7_Values = pd.DataFrame(X_7_Values)\n",
    "X_7_Values.columns = Cols\n",
    "X_7_Values.to_csv('F7_X_Values.csv')\n",
    "\n",
    "X_9_Values = pd.DataFrame(X_9_Values)\n",
    "X_9_Values.columns = Cols\n",
    "X_9_Values.to_csv('F9_X_Values.csv')\n",
    "\n",
    "X_10_Values = pd.DataFrame(X_10_Values)\n",
    "X_10_Values.columns = Cols\n",
    "X_10_Values.to_csv('F10_X_Values.csv')\n",
    "\n",
    "X_13_Values = pd.DataFrame(X_13_Values)\n",
    "X_13_Values.columns = Cols\n",
    "X_13_Values.to_csv('F13_X_Values.csv')\n",
    "\n",
    "X_15_Values = pd.DataFrame(X_15_Values)\n",
    "X_15_Values.columns = Cols\n",
    "X_15_Values.to_csv('F15_X_Values.csv')\n",
    "\n",
    "X_16_Values = pd.DataFrame(X_16_Values)\n",
    "X_16_Values.columns = Cols\n",
    "X_16_Values.to_csv('F16_X_Values.csv')\n",
    "\n",
    "X_20_Values = pd.DataFrame(X_20_Values)\n",
    "X_20_Values.columns = Cols\n",
    "X_20_Values.to_csv('F20_X_Values.csv')\n",
    "\n",
    "X_24_Values = pd.DataFrame(X_24_Values)\n",
    "X_24_Values.columns = Cols\n",
    "X_24_Values.to_csv('F24_X_Values.csv')\n",
    "\n",
    "\n",
    "Krig_Fun_2 = np.zeros(30)\n",
    "Krig_Fun_3 = np.zeros(30)\n",
    "Krig_Fun_7 = np.zeros(30)\n",
    "Krig_Fun_9 = np.zeros(30)\n",
    "Krig_Fun_10 = np.zeros(30)\n",
    "Krig_Fun_13 = np.zeros(30)\n",
    "Krig_Fun_15 = np.zeros(30)\n",
    "Krig_Fun_16 = np.zeros(30)\n",
    "Krig_Fun_20 = np.zeros(30)\n",
    "Krig_Fun_24 = np.zeros(30)\n",
    "for i in range(X_2_Values.shape[0]):\n",
    "    Krig_Fun_2 [i] = F2(X_2_Values.iloc[i,:])\n",
    "    Krig_Fun_3 [i] = F3(X_3_Values.iloc[i,:])\n",
    "    Krig_Fun_7 [i] = F7(X_7_Values.iloc[i,:])\n",
    "    Krig_Fun_9 [i] = F9(X_9_Values.iloc[i,:])\n",
    "    Krig_Fun_10 [i] = F10(X_10_Values.iloc[i,:])\n",
    "    Krig_Fun_13 [i] = F13(X_13_Values.iloc[i,:])\n",
    "    Krig_Fun_15 [i] = F15(X_15_Values.iloc[i,:])\n",
    "    Krig_Fun_16 [i] = F16(X_16_Values.iloc[i,:])\n",
    "    Krig_Fun_20 [i] = F20(X_20_Values.iloc[i,:])\n",
    "    Krig_Fun_24 [i] = F24(X_24_Values.iloc[i,:])\n",
    "    \n",
    "\n",
    "print ('Kriging-f2')\n",
    "print (np.median(Krig_Fun_2) )\n",
    "print ('Kriging-f3')\n",
    "print (np.median(Krig_Fun_3) )\n",
    "print ('Kriging-f7')\n",
    "print (np.median(Krig_Fun_7) )\n",
    "print ('Kriging-f9')\n",
    "print (np.median(Krig_Fun_9) )\n",
    "print ('Kriging-f10')\n",
    "print (np.median(Krig_Fun_10) )\n",
    "print ('Kriging-f13')\n",
    "print (np.median(Krig_Fun_13) )\n",
    "print ('Kriging-f15')\n",
    "print (np.median(Krig_Fun_15) )\n",
    "print ('Kriging-f16')\n",
    "print (np.median(Krig_Fun_16) )\n",
    "print ('Kriging-f20')\n",
    "print (np.median(Krig_Fun_20) )\n",
    "print ('Kriging-f24')\n",
    "print (np.median(Krig_Fun_24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27493603e+07,  4.59251553e+02,  2.46665358e+02,  3.76755044e+05,\n",
       "        2.09211091e+06,  6.64847201e+02,  4.94068849e+02, -1.87977940e+02,\n",
       "        7.29439206e+04,  2.39855945e+02])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.median(Krig_Fun_2),np.median(Krig_Fun_3),np.median(Krig_Fun_7),np.median(Krig_Fun_9),\n",
    "np.median(Krig_Fun_10),np.median(Krig_Fun_13),np.median(Krig_Fun_15),np.median(Krig_Fun_16),\n",
    "np.median(Krig_Fun_20),np.median(Krig_Fun_24)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
