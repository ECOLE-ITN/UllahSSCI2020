{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.3.0+cpu cuda: False\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch.utils.data\n",
    "import torch.nn.init as init\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyDOE\n",
    "from scipy import stats\n",
    "import sys\n",
    "import scipy.stats.distributions as dist\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from collections import namedtuple\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "print( 'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dimensionality Reduction Related Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Re-Scale Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Give the path to the training input files as the function argument, the function loads and rescales the initial data set\"\"\"\n",
    "def load_rescale_data_sets(path):\n",
    "    train_data = pd.read_csv(path, index_col=0).iloc[:,:-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_data = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_2_200Samples.csv'), index_col=0).iloc[:,:-1]\n",
    "    cols = test_data.columns\n",
    "    scalar = MinMaxScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(scalar.transform (train_data)) \n",
    "    test_data = pd.DataFrame(scalar.transform (test_data)) \n",
    "    train_data.columns, test_data.columns = cols, cols\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Class Variational Autoencoders, which implements the Variational Autoencoders \n",
    "Neural Networks for Dimensionality Reduction \"\"\"\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim, z_dim):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.h1_dim = int (self.x_dim - (1/5) * (self.x_dim - self.z_dim))\n",
    "        self.h2_dim = int (self.x_dim - (2/5) * (self.x_dim - self.z_dim))\n",
    "        self.h3_dim = int (self.x_dim - (3/5) * (self.x_dim - self.z_dim))\n",
    "        self.h4_dim = int (self.x_dim - (4/5) * (self.x_dim - self.z_dim))\n",
    "        \n",
    "\n",
    "        #encoder\n",
    "        self.enc = nn.Sequential( nn.Linear(self.x_dim , self.h1_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h1_dim , self.h2_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h2_dim , self.h3_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h3_dim , self.h4_dim), nn.Tanh())\n",
    "        \n",
    "        self.enc_mean = nn.Sequential( nn.Linear(self.h4_dim, self.z_dim) )\n",
    "        self.enc_std = nn.Sequential( nn.Linear(self.h4_dim, self.z_dim))\n",
    "\n",
    "        #decoder\n",
    "        self.dec = nn.Sequential(nn.Linear(self.z_dim , self.h4_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h4_dim , self.h3_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h3_dim , self.h2_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h2_dim , self.h1_dim), nn.Tanh())\n",
    "        \n",
    "        self.dec_mean = nn.Sequential( nn.Linear( self.h1_dim, self.x_dim ))\n",
    "        self.dec_std = nn.Sequential( nn.Linear(self.h1_dim, self.x_dim))\n",
    "        \n",
    "\n",
    "    def encode (self, x ):\n",
    "        #print (x.shape)\n",
    "        enc = self.enc(x.float())\n",
    "        enc_mean = self.enc_mean(enc)\n",
    "        enc_std = self.enc_std(enc)\n",
    "        return enc_mean , enc_std\n",
    "\n",
    "\n",
    "    def decode (self, z):\n",
    "        dec = self.dec(z)\n",
    "        dec_mean = self.dec_mean(dec)\n",
    "        dec_std = self.dec_std(dec)\n",
    "        return dec_mean , dec_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "        #encoder\n",
    "        enc_mean , enc_std = self.encode(x)\n",
    "        #sampling and reparameterization\n",
    "        z = self._reparameterized_sample(enc_mean, enc_std)\n",
    "        #decoder\n",
    "        dec_mean , dec_std = self.decode(z)\n",
    "        kld_loss += self._kld_gauss(enc_mean, enc_std.mul(0.5).exp_())\n",
    "        nll_loss += self._nll_gauss(dec_mean, dec_std, x)\n",
    "        return kld_loss, nll_loss,(enc_mean , enc_std),(dec_mean , dec_std) , z\n",
    "\n",
    "\n",
    "    def _reparameterized_sample(self, mean, logvar):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "    def _kld_gauss(self, mu, logvar):\n",
    "        \"\"\"Using std to compute KLD\"\"\"\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    def _nll_gauss(self, mean, logvar , x):\n",
    "        return torch.sum( 0.5 * np.log (2 * np.pi) + 0.5 * logvar + (x-mean)**2 / (2 *  torch.exp(logvar)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions to Implement the Training and Testing of AEs, Based on Methods in Autoencoder Class\"\"\"\n",
    "def train(epoch, train_loader, train_data, batch_size, model):\n",
    "    train_loss = 0\n",
    "    epoch_loss = np.zeros(int(len (train_data) / batch_size ))\n",
    "    epoch_div = np.zeros(int(len (train_data) / batch_size))\n",
    "    clip, learning_rate, seed, print_every, save_every  = 10, 1e-3 , 100, 10, 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for batch_idx, (data) in enumerate(train_loader):\n",
    "        \n",
    "        data = Variable(data)\n",
    "        #forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        kld_loss, nll_loss, lat, recon, _ = model(data)\n",
    "        epoch_loss [batch_idx] = nll_loss\n",
    "        epoch_div [batch_idx] = kld_loss\n",
    "        loss = kld_loss + nll_loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        #printing\n",
    "        if batch_idx % print_every == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t KLD Loss: {:.6f} \\t NLL Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                kld_loss.data / batch_size,\n",
    "                nll_loss.data / batch_size))\n",
    "\n",
    "            \n",
    "\n",
    "        train_loss += loss.data\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "    return epoch_loss, epoch_div, model\n",
    "    \n",
    "def test(epoch, test_loader, test_data, model):\n",
    "    \"\"\"uses test data to evaluate \n",
    "    likelihood of the model\"\"\"\n",
    "    mean_kld_loss, mean_nll_loss = 0, 0\n",
    "    epoch_loss = np.zeros(len(test_data))\n",
    "    epoch_div = np.zeros(len(test_data))\n",
    "    for i, (data) in enumerate(test_loader):                                           \n",
    "        \n",
    "        data = Variable(data.reshape(1,-1))\n",
    "        kld_loss, nll_loss, _, _, _ = model(data)\n",
    "        epoch_div [i] = kld_loss\n",
    "        epoch_loss [i] = nll_loss\n",
    "        mean_kld_loss += kld_loss.data\n",
    "        mean_nll_loss += nll_loss.data\n",
    "\n",
    "    mean_kld_loss /= len(test_loader.dataset)\n",
    "    mean_nll_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('====> Test set loss: KLD Loss = {:.4f}, NLL Loss = {:.4f} '.format(\n",
    "        mean_kld_loss, mean_nll_loss))\n",
    "    return epoch_loss, epoch_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is the method that implements the dimensionality reduction based on Autoencoders \"\"\"\n",
    "def perform_dimensionality_reduction (hyper_paras, path):\n",
    "    seed = 100\n",
    "    # Change the 0.3 to 0.6 and 0.9 for 60 % and 90 % dimensionality reduction\n",
    "    z_dim = int(50-0.3 * 50) # 30 % dimensionality reduction\n",
    "    n_epochs, batch_size = hyper_paras\n",
    "    train_data, test_data = load_rescale_data_sets(path)\n",
    "    x_dim = train_data.shape[1]\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    h1_dim = int (x_dim - (1/5) * (x_dim - z_dim))\n",
    "    h2_dim = int (x_dim - (2/5) * (x_dim - z_dim))\n",
    "    h3_dim = int (x_dim - (3/5) * (x_dim - z_dim))\n",
    "    h4_dim = int (x_dim - (4/5) * (x_dim - z_dim))\n",
    "    \n",
    "    print (x_dim, h1_dim, h2_dim, h3_dim,h4_dim,z_dim)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader ( dataset = train_data.values ,  batch_size = batch_size , shuffle= True)\n",
    "    test_loader = torch.utils.data.DataLoader (  dataset = test_data.values , shuffle= True)\n",
    "    train_error = np.zeros([n_epochs , int(train_data.shape[0] / batch_size ) ])\n",
    "    train_div = np.zeros([n_epochs , int(train_data.shape[0] / batch_size ) ])\n",
    "    test_error , test_div  = np.zeros([n_epochs , test_data.shape[0]]) , np.zeros([n_epochs , test_data.shape[0]]) \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        #training + testing\n",
    "        if (epoch==1): ## only for the first time, take the default model, all the next times in iteration, keep improving it\n",
    "            model = VAE(x_dim, z_dim)\n",
    "            \n",
    "        tr = train(epoch, train_loader, train_data, batch_size, model)\n",
    "        train_error [epoch-1 , :] = tr [0]\n",
    "        train_div [epoch-1 , :] = tr [1] \n",
    "        model = tr[2]\n",
    "        te = test(epoch, test_loader, test_data, model)\n",
    "        test_error [epoch-1 , :] = te [0]\n",
    "        test_div [epoch-1 , :] = te [1]\n",
    "            \n",
    "    train_lat = [ model (Variable(torch.tensor(train_data.iloc[idx,:].values)).reshape(1,-1))[-1] for idx in range(len(train_data)) ]\n",
    "    test_lat = [ model (Variable(torch.tensor(test_data.iloc[idx,:].values)).reshape(1,-1))[-1] for idx in range(len(test_data)) ]\n",
    "    train_lat = pd.DataFrame(torch.cat(train_lat).cpu().detach().numpy())\n",
    "    test_lat = pd.DataFrame(torch.cat(test_lat).cpu().detach().numpy())\n",
    "    cols = []\n",
    "    for i in range(train_lat.shape[1]):\n",
    "        cols.append(str('Z'+str(i+1)))\n",
    "    train_lat.columns = cols\n",
    "    test_lat.columns = cols\n",
    "    train_lat.to_csv('VAEs_ELN_50D_30%_latent_training.csv') # Change the path here depending upon dimensionality and reduction\n",
    "    test_lat.to_csv('VAEs_ELN_50D_30%_latent_test.csv') # Same here\n",
    "    return train_error, train_div, test_error, test_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Dimensionality Reduction Related Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New Reduced Data Sets for all Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_f2(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_2_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f3(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_3_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f7(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_7_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f9(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_9_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f10(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets\\\\test_10_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f13(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets\\\\test_13_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f15(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets\\\\test_15_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f16(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets\\\\test_16_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f20(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_20_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f24(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets\\\\test_24_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Kriging Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Elastic Net Regression '''\n",
    "def elastic_net(train_data,test_data,hyper):\n",
    "    alp, rat = hyper\n",
    "    scaler =  MinMaxScaler().fit(np.r_[train_data.iloc[:,:-1].values])\n",
    "    regr = ElasticNet(alpha= np.power(10,alp) ,random_state=0 , l1_ratio=rat, fit_intercept =True, max_iter=3000,selection='random').fit(scaler.transform ( np.array(train_data.iloc[:,:-1])) ,  np.array(train_data.iloc[:,-1]))\n",
    "    pred = regr.predict(scaler.transform(test_data))\n",
    "    return regr,pred\n",
    "\n",
    "\"\"\" Generating Polynomial Features i.e., Function Basis \"\"\"\n",
    "def quadratic_polynomial (df):\n",
    "    return pd.DataFrame(PolynomialFeatures(degree=2).fit_transform(df))\n",
    "\n",
    "\"\"\" Quadratic Regression with Elastic Net Penalty\"\"\"\n",
    "def polynomial(tr, te,hyper):\n",
    "    f_original = tr['Y']\n",
    "    temp1 = quadratic_polynomial (tr.iloc[:,:-1])\n",
    "    temp2 = quadratic_polynomial (te.iloc[:,:-1])\n",
    "    temp1 ['Y'] = f_original\n",
    "    model_eln , pred_eln = elastic_net(temp1,temp2,hyper)\n",
    "    return model_eln , pred_eln\n",
    "\n",
    "\n",
    "\"\"\" Normalized Mean Absolute Error % \"\"\"\n",
    "def rmae(true, pred):\n",
    "    return np.mean((abs(true-pred) / abs(true)) * 100)\n",
    "\n",
    "\"\"\" This method implements and evaluates the Polynomial Surrogate Model with RMAE \"\"\"\n",
    "def surrogate_model(train_data,test_data,hyper,true):\n",
    "    model_eln , pred_eln = polynomial (train_data,test_data,hyper)\n",
    "    return rmae(true,pred_eln)\n",
    "\n",
    "\"\"\" Implements all the surrogate models, i.e., for all test function, and returns the median of RMAE errors,\n",
    "This median is used as the primary metric for Hyper-Parameters Optimization \"\"\"\n",
    "def perform_surrogate_modeling(paths, path_latent_train,path_latent_test,hyper):\n",
    "    train_2, test_2, true_2 = load_f2(paths[0],path_latent_train,path_latent_test)\n",
    "    rmae_2 = surrogate_model(train_2, test_2,hyper, true_2)\n",
    "    \n",
    "    train_3, test_3, true_3 = load_f3(paths[1],path_latent_train,path_latent_test)\n",
    "    rmae_3 = surrogate_model(train_3, test_3,hyper, true_3)\n",
    "    \n",
    "    train_7, test_7, true_7 = load_f7(paths[2],path_latent_train,path_latent_test)\n",
    "    rmae_7 = surrogate_model(train_7, test_7,hyper, true_7)\n",
    "    \n",
    "    train_9, test_9, true_9 = load_f9(paths[3],path_latent_train,path_latent_test)\n",
    "    rmae_9 = surrogate_model(train_9, test_9,hyper, true_9)\n",
    "    \n",
    "    train_10, test_10, true_10 = load_f10(paths[4],path_latent_train,path_latent_test)\n",
    "    rmae_10 = surrogate_model(train_10, test_10,hyper, true_10)\n",
    "    \n",
    "    train_13, test_13, true_13 = load_f13(paths[5],path_latent_train,path_latent_test)\n",
    "    rmae_13 = surrogate_model(train_13, test_13,hyper, true_13)\n",
    "    \n",
    "    train_15, test_15, true_15 = load_f15(paths[6],path_latent_train,path_latent_test)\n",
    "    rmae_15 = surrogate_model(train_15, test_15,hyper, true_15)\n",
    "    \n",
    "    train_16, test_16, true_16 = load_f16(paths[7],path_latent_train,path_latent_test)\n",
    "    rmae_16 = surrogate_model(train_16, test_16,hyper, true_16)\n",
    "    \n",
    "    train_20, test_20, true_20 = load_f20(paths[8],path_latent_train,path_latent_test)\n",
    "    rmae_20 = surrogate_model(train_20, test_20,hyper, true_20)\n",
    "    \n",
    "    train_24, test_24, true_24 = load_f24(paths[9],path_latent_train,path_latent_test)\n",
    "    rmae_24 = surrogate_model(train_24, test_24,hyper, true_24)\n",
    "    \n",
    "    accuracy = [rmae_2,rmae_3,rmae_7,rmae_9,rmae_10,rmae_13,rmae_15,rmae_16,rmae_20,rmae_24]\n",
    "    return accuracy\n",
    "\n",
    "\"\"\" This is the function used for Hyper_Parameters_Optimization for both dimensionality reduction and surrogate modelling \"\"\"\n",
    "def hyper_parameters_optimization(hyper_dim,path_latent_train,path_latent_test,hyper):\n",
    "    print ('Start Dimensionality Reduction:::')\n",
    "    _ , _, _, _ = perform_dimensionality_reduction (hyper_dim, paths[0])\n",
    "    print ('End Dimensionality Reduction:::')\n",
    "    accuracy = perform_surrogate_modeling (paths,path_latent_train,path_latent_test,hyper)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000}\n",
    "path_2 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_2_1000Samples.csv\"\n",
    "path_3 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_3_1000Samples.csv\"\n",
    "path_7 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_7_1000Samples.csv\"\n",
    "path_9 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_9_1000Samples.csv\"\n",
    "path_10 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_10_1000Samples.csv\"\n",
    "path_13 = \"50 D\\\\Training_Data_Sets\\\\train_13_1000Samples.csv\"\n",
    "path_15 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_15_1000Samples.csv\"\n",
    "path_16 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_16_1000Samples.csv\"\n",
    "path_20 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_201000Samples.csv\"\n",
    "path_24 = \"Data Generation\\\\50 D\\\\Training_Data_Sets\\\\train_241000Samples.csv\"\n",
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \n",
    "path_latent_train = \"VAEs_ELN_50D_30%_latent_training.csv\"\n",
    "path_latent_test = \"VAEs_ELN_50D_30%_latent_test.csv\"\n",
    "paths = [path_2,path_3,path_7,path_9,path_10,path_13,path_15,path_16,path_20,path_24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Dimensionality Reduction:::\n",
      "50 47 44 41 38 35\n",
      "Train Epoch: 1 [0/1000 (0%)]\t KLD Loss: 12.752107 \t NLL Loss: 55.766216\n",
      "Train Epoch: 1 [200/1000 (20%)]\t KLD Loss: 10.767653 \t NLL Loss: 51.563186\n",
      "Train Epoch: 1 [400/1000 (40%)]\t KLD Loss: 7.702577 \t NLL Loss: 45.765576\n",
      "Train Epoch: 1 [600/1000 (60%)]\t KLD Loss: 5.452963 \t NLL Loss: 35.229510\n",
      "Train Epoch: 1 [800/1000 (80%)]\t KLD Loss: 3.099154 \t NLL Loss: 26.002713\n",
      "====> Epoch: 1 Average loss: 46.3554\n",
      "====> Test set loss: KLD Loss = 2.9014, NLL Loss = 15.1576 \n",
      "Train Epoch: 2 [0/1000 (0%)]\t KLD Loss: 2.901012 \t NLL Loss: 14.477570\n",
      "Train Epoch: 2 [200/1000 (20%)]\t KLD Loss: 1.590598 \t NLL Loss: 11.300395\n",
      "Train Epoch: 2 [400/1000 (40%)]\t KLD Loss: 0.962459 \t NLL Loss: 9.590695\n",
      "Train Epoch: 2 [600/1000 (60%)]\t KLD Loss: 0.657704 \t NLL Loss: 8.305398\n",
      "Train Epoch: 2 [800/1000 (80%)]\t KLD Loss: 0.546531 \t NLL Loss: 9.073419\n",
      "====> Epoch: 2 Average loss: 11.4378\n",
      "====> Test set loss: KLD Loss = 0.4159, NLL Loss = 9.5587 \n",
      "Train Epoch: 3 [0/1000 (0%)]\t KLD Loss: 0.415870 \t NLL Loss: 10.157184\n",
      "Train Epoch: 3 [200/1000 (20%)]\t KLD Loss: 0.303088 \t NLL Loss: 9.915526\n",
      "Train Epoch: 3 [400/1000 (40%)]\t KLD Loss: 0.199701 \t NLL Loss: 9.378795\n",
      "Train Epoch: 3 [600/1000 (60%)]\t KLD Loss: 0.150170 \t NLL Loss: 10.100606\n",
      "Train Epoch: 3 [800/1000 (80%)]\t KLD Loss: 0.119093 \t NLL Loss: 9.466138\n",
      "====> Epoch: 3 Average loss: 9.7315\n",
      "====> Test set loss: KLD Loss = 0.1014, NLL Loss = 9.4066 \n",
      "Train Epoch: 4 [0/1000 (0%)]\t KLD Loss: 0.101356 \t NLL Loss: 9.436281\n",
      "Train Epoch: 4 [200/1000 (20%)]\t KLD Loss: 0.066878 \t NLL Loss: 9.810674\n",
      "Train Epoch: 4 [400/1000 (40%)]\t KLD Loss: 0.061217 \t NLL Loss: 8.410544\n",
      "Train Epoch: 4 [600/1000 (60%)]\t KLD Loss: 0.051444 \t NLL Loss: 10.519785\n",
      "Train Epoch: 4 [800/1000 (80%)]\t KLD Loss: 0.043087 \t NLL Loss: 10.752259\n",
      "====> Epoch: 4 Average loss: 9.5564\n",
      "====> Test set loss: KLD Loss = 0.0402, NLL Loss = 9.4493 \n",
      "Train Epoch: 5 [0/1000 (0%)]\t KLD Loss: 0.040163 \t NLL Loss: 9.234413\n",
      "Train Epoch: 5 [200/1000 (20%)]\t KLD Loss: 0.032737 \t NLL Loss: 8.518070\n",
      "Train Epoch: 5 [400/1000 (40%)]\t KLD Loss: 0.031601 \t NLL Loss: 10.378157\n",
      "Train Epoch: 5 [600/1000 (60%)]\t KLD Loss: 0.051939 \t NLL Loss: 9.676876\n",
      "Train Epoch: 5 [800/1000 (80%)]\t KLD Loss: 0.031449 \t NLL Loss: 9.440366\n",
      "====> Epoch: 5 Average loss: 9.4725\n",
      "====> Test set loss: KLD Loss = 0.0243, NLL Loss = 9.4577 \n",
      "Train Epoch: 6 [0/1000 (0%)]\t KLD Loss: 0.024255 \t NLL Loss: 9.819688\n",
      "Train Epoch: 6 [200/1000 (20%)]\t KLD Loss: 0.020494 \t NLL Loss: 9.070198\n",
      "Train Epoch: 6 [400/1000 (40%)]\t KLD Loss: 0.018918 \t NLL Loss: 8.192940\n",
      "Train Epoch: 6 [600/1000 (60%)]\t KLD Loss: 0.015971 \t NLL Loss: 10.275183\n",
      "Train Epoch: 6 [800/1000 (80%)]\t KLD Loss: 0.011468 \t NLL Loss: 10.575846\n",
      "====> Epoch: 6 Average loss: 9.4494\n",
      "====> Test set loss: KLD Loss = 0.0075, NLL Loss = 9.3415 \n",
      "Train Epoch: 7 [0/1000 (0%)]\t KLD Loss: 0.007541 \t NLL Loss: 8.388972\n",
      "Train Epoch: 7 [200/1000 (20%)]\t KLD Loss: 0.009524 \t NLL Loss: 8.936398\n",
      "Train Epoch: 7 [400/1000 (40%)]\t KLD Loss: 0.010446 \t NLL Loss: 10.181186\n",
      "Train Epoch: 7 [600/1000 (60%)]\t KLD Loss: 0.011653 \t NLL Loss: 9.979510\n",
      "Train Epoch: 7 [800/1000 (80%)]\t KLD Loss: 0.016539 \t NLL Loss: 9.706781\n",
      "====> Epoch: 7 Average loss: 9.4176\n",
      "====> Test set loss: KLD Loss = 0.0140, NLL Loss = 9.4383 \n",
      "Train Epoch: 8 [0/1000 (0%)]\t KLD Loss: 0.014024 \t NLL Loss: 9.060479\n",
      "Train Epoch: 8 [200/1000 (20%)]\t KLD Loss: 0.017007 \t NLL Loss: 10.629315\n",
      "Train Epoch: 8 [400/1000 (40%)]\t KLD Loss: 0.024795 \t NLL Loss: 8.917308\n",
      "Train Epoch: 8 [600/1000 (60%)]\t KLD Loss: 0.023830 \t NLL Loss: 9.360989\n",
      "Train Epoch: 8 [800/1000 (80%)]\t KLD Loss: 0.020427 \t NLL Loss: 10.158128\n",
      "====> Epoch: 8 Average loss: 9.4268\n",
      "====> Test set loss: KLD Loss = 0.0134, NLL Loss = 9.4780 \n",
      "Train Epoch: 9 [0/1000 (0%)]\t KLD Loss: 0.013402 \t NLL Loss: 9.893981\n",
      "Train Epoch: 9 [200/1000 (20%)]\t KLD Loss: 0.022589 \t NLL Loss: 10.141763\n",
      "Train Epoch: 9 [400/1000 (40%)]\t KLD Loss: 0.015107 \t NLL Loss: 8.465752\n",
      "Train Epoch: 9 [600/1000 (60%)]\t KLD Loss: 0.015328 \t NLL Loss: 8.401743\n",
      "Train Epoch: 9 [800/1000 (80%)]\t KLD Loss: 0.022377 \t NLL Loss: 10.286678\n",
      "====> Epoch: 9 Average loss: 9.4051\n",
      "====> Test set loss: KLD Loss = 0.0231, NLL Loss = 9.3286 \n",
      "Train Epoch: 10 [0/1000 (0%)]\t KLD Loss: 0.023057 \t NLL Loss: 10.448623\n",
      "Train Epoch: 10 [200/1000 (20%)]\t KLD Loss: 0.012505 \t NLL Loss: 10.129852\n",
      "Train Epoch: 10 [400/1000 (40%)]\t KLD Loss: 0.014635 \t NLL Loss: 9.177471\n",
      "Train Epoch: 10 [600/1000 (60%)]\t KLD Loss: 0.014838 \t NLL Loss: 10.215077\n",
      "Train Epoch: 10 [800/1000 (80%)]\t KLD Loss: 0.009615 \t NLL Loss: 9.128756\n",
      "====> Epoch: 10 Average loss: 9.3245\n",
      "====> Test set loss: KLD Loss = 0.0124, NLL Loss = 9.3265 \n",
      "Train Epoch: 11 [0/1000 (0%)]\t KLD Loss: 0.012323 \t NLL Loss: 8.841425\n",
      "Train Epoch: 11 [200/1000 (20%)]\t KLD Loss: 0.016494 \t NLL Loss: 8.780341\n",
      "Train Epoch: 11 [400/1000 (40%)]\t KLD Loss: 0.026660 \t NLL Loss: 11.030871\n",
      "Train Epoch: 11 [600/1000 (60%)]\t KLD Loss: 0.027603 \t NLL Loss: 8.805955\n",
      "Train Epoch: 11 [800/1000 (80%)]\t KLD Loss: 0.008540 \t NLL Loss: 9.279429\n",
      "====> Epoch: 11 Average loss: 9.3259\n",
      "====> Test set loss: KLD Loss = 0.0075, NLL Loss = 9.4015 \n",
      "Train Epoch: 12 [0/1000 (0%)]\t KLD Loss: 0.007640 \t NLL Loss: 9.464473\n",
      "Train Epoch: 12 [200/1000 (20%)]\t KLD Loss: 0.037448 \t NLL Loss: 8.168363\n",
      "Train Epoch: 12 [400/1000 (40%)]\t KLD Loss: 0.022313 \t NLL Loss: 8.826739\n",
      "Train Epoch: 12 [600/1000 (60%)]\t KLD Loss: 0.031747 \t NLL Loss: 9.223417\n",
      "Train Epoch: 12 [800/1000 (80%)]\t KLD Loss: 0.042912 \t NLL Loss: 9.332090\n",
      "====> Epoch: 12 Average loss: 9.3775\n",
      "====> Test set loss: KLD Loss = 0.0262, NLL Loss = 9.3584 \n",
      "Train Epoch: 13 [0/1000 (0%)]\t KLD Loss: 0.026789 \t NLL Loss: 8.938485\n",
      "Train Epoch: 13 [200/1000 (20%)]\t KLD Loss: 0.070439 \t NLL Loss: 8.744917\n",
      "Train Epoch: 13 [400/1000 (40%)]\t KLD Loss: 0.054512 \t NLL Loss: 10.068604\n",
      "Train Epoch: 13 [600/1000 (60%)]\t KLD Loss: 0.049208 \t NLL Loss: 9.196254\n",
      "Train Epoch: 13 [800/1000 (80%)]\t KLD Loss: 0.037723 \t NLL Loss: 9.273872\n",
      "====> Epoch: 13 Average loss: 9.1126\n",
      "====> Test set loss: KLD Loss = 0.0506, NLL Loss = 9.0432 \n",
      "Train Epoch: 14 [0/1000 (0%)]\t KLD Loss: 0.055234 \t NLL Loss: 9.093873\n",
      "Train Epoch: 14 [200/1000 (20%)]\t KLD Loss: 0.057437 \t NLL Loss: 8.549547\n",
      "Train Epoch: 14 [400/1000 (40%)]\t KLD Loss: 0.055410 \t NLL Loss: 9.380834\n",
      "Train Epoch: 14 [600/1000 (60%)]\t KLD Loss: 0.046145 \t NLL Loss: 9.439674\n",
      "Train Epoch: 14 [800/1000 (80%)]\t KLD Loss: 0.013365 \t NLL Loss: 9.443816\n",
      "====> Epoch: 14 Average loss: 8.9294\n",
      "====> Test set loss: KLD Loss = 0.0235, NLL Loss = 8.8690 \n",
      "Train Epoch: 15 [0/1000 (0%)]\t KLD Loss: 0.034439 \t NLL Loss: 9.269934\n",
      "Train Epoch: 15 [200/1000 (20%)]\t KLD Loss: 0.033893 \t NLL Loss: 8.997523\n",
      "Train Epoch: 15 [400/1000 (40%)]\t KLD Loss: 0.040230 \t NLL Loss: 9.117476\n",
      "Train Epoch: 15 [600/1000 (60%)]\t KLD Loss: 0.042748 \t NLL Loss: 7.913740\n",
      "Train Epoch: 15 [800/1000 (80%)]\t KLD Loss: 0.035926 \t NLL Loss: 8.801861\n",
      "====> Epoch: 15 Average loss: 8.9512\n",
      "====> Test set loss: KLD Loss = 0.0305, NLL Loss = 8.9256 \n",
      "Train Epoch: 16 [0/1000 (0%)]\t KLD Loss: 0.042245 \t NLL Loss: 7.246647\n",
      "Train Epoch: 16 [200/1000 (20%)]\t KLD Loss: 0.028140 \t NLL Loss: 9.736446\n",
      "Train Epoch: 16 [400/1000 (40%)]\t KLD Loss: 0.040311 \t NLL Loss: 9.117942\n",
      "Train Epoch: 16 [600/1000 (60%)]\t KLD Loss: 0.039425 \t NLL Loss: 7.814595\n",
      "Train Epoch: 16 [800/1000 (80%)]\t KLD Loss: 0.026004 \t NLL Loss: 9.366018\n",
      "====> Epoch: 16 Average loss: 8.8578\n",
      "====> Test set loss: KLD Loss = 0.0230, NLL Loss = 8.8391 \n",
      "Train Epoch: 17 [0/1000 (0%)]\t KLD Loss: 0.025529 \t NLL Loss: 8.763499\n",
      "Train Epoch: 17 [200/1000 (20%)]\t KLD Loss: 0.033356 \t NLL Loss: 8.106967\n",
      "Train Epoch: 17 [400/1000 (40%)]\t KLD Loss: 0.040670 \t NLL Loss: 9.727435\n",
      "Train Epoch: 17 [600/1000 (60%)]\t KLD Loss: 0.028829 \t NLL Loss: 8.079646\n",
      "Train Epoch: 17 [800/1000 (80%)]\t KLD Loss: 0.023102 \t NLL Loss: 8.757306\n",
      "====> Epoch: 17 Average loss: 8.8362\n",
      "====> Test set loss: KLD Loss = 0.0243, NLL Loss = 8.9187 \n",
      "Train Epoch: 18 [0/1000 (0%)]\t KLD Loss: 0.021367 \t NLL Loss: 8.613948\n",
      "Train Epoch: 18 [200/1000 (20%)]\t KLD Loss: 0.017986 \t NLL Loss: 9.885303\n",
      "Train Epoch: 18 [400/1000 (40%)]\t KLD Loss: 0.029935 \t NLL Loss: 8.605156\n",
      "Train Epoch: 18 [600/1000 (60%)]\t KLD Loss: 0.045333 \t NLL Loss: 9.020163\n",
      "Train Epoch: 18 [800/1000 (80%)]\t KLD Loss: 0.032525 \t NLL Loss: 7.873856\n",
      "====> Epoch: 18 Average loss: 8.8218\n",
      "====> Test set loss: KLD Loss = 0.0212, NLL Loss = 9.0145 \n",
      "Train Epoch: 19 [0/1000 (0%)]\t KLD Loss: 0.027699 \t NLL Loss: 8.873844\n",
      "Train Epoch: 19 [200/1000 (20%)]\t KLD Loss: 0.036124 \t NLL Loss: 8.446535\n",
      "Train Epoch: 19 [400/1000 (40%)]\t KLD Loss: 0.045198 \t NLL Loss: 9.331532\n",
      "Train Epoch: 19 [600/1000 (60%)]\t KLD Loss: 0.037114 \t NLL Loss: 8.774528\n",
      "Train Epoch: 19 [800/1000 (80%)]\t KLD Loss: 0.035528 \t NLL Loss: 8.860136\n",
      "====> Epoch: 19 Average loss: 8.8167\n",
      "====> Test set loss: KLD Loss = 0.0228, NLL Loss = 8.9448 \n",
      "Train Epoch: 20 [0/1000 (0%)]\t KLD Loss: 0.016170 \t NLL Loss: 8.792835\n",
      "Train Epoch: 20 [200/1000 (20%)]\t KLD Loss: 0.030472 \t NLL Loss: 8.106961\n",
      "Train Epoch: 20 [400/1000 (40%)]\t KLD Loss: 0.039519 \t NLL Loss: 9.803447\n",
      "Train Epoch: 20 [600/1000 (60%)]\t KLD Loss: 0.038517 \t NLL Loss: 7.493630\n",
      "Train Epoch: 20 [800/1000 (80%)]\t KLD Loss: 0.034658 \t NLL Loss: 8.158503\n",
      "====> Epoch: 20 Average loss: 8.7991\n",
      "====> Test set loss: KLD Loss = 0.0233, NLL Loss = 8.8468 \n",
      "Train Epoch: 21 [0/1000 (0%)]\t KLD Loss: 0.021611 \t NLL Loss: 7.951646\n",
      "Train Epoch: 21 [200/1000 (20%)]\t KLD Loss: 0.030941 \t NLL Loss: 9.407400\n",
      "Train Epoch: 21 [400/1000 (40%)]\t KLD Loss: 0.048022 \t NLL Loss: 8.979386\n",
      "Train Epoch: 21 [600/1000 (60%)]\t KLD Loss: 0.034349 \t NLL Loss: 8.581830\n",
      "Train Epoch: 21 [800/1000 (80%)]\t KLD Loss: 0.035036 \t NLL Loss: 9.455638\n",
      "====> Epoch: 21 Average loss: 8.6954\n",
      "====> Test set loss: KLD Loss = 0.0381, NLL Loss = 8.8774 \n",
      "Train Epoch: 22 [0/1000 (0%)]\t KLD Loss: 0.051044 \t NLL Loss: 8.252268\n",
      "Train Epoch: 22 [200/1000 (20%)]\t KLD Loss: 0.029745 \t NLL Loss: 8.354110\n",
      "Train Epoch: 22 [400/1000 (40%)]\t KLD Loss: 0.029985 \t NLL Loss: 7.443270\n",
      "Train Epoch: 22 [600/1000 (60%)]\t KLD Loss: 0.028032 \t NLL Loss: 9.316664\n",
      "Train Epoch: 22 [800/1000 (80%)]\t KLD Loss: 0.032187 \t NLL Loss: 8.985085\n",
      "====> Epoch: 22 Average loss: 8.7160\n",
      "====> Test set loss: KLD Loss = 0.0399, NLL Loss = 8.7989 \n",
      "Train Epoch: 23 [0/1000 (0%)]\t KLD Loss: 0.045673 \t NLL Loss: 8.507533\n",
      "Train Epoch: 23 [200/1000 (20%)]\t KLD Loss: 0.036419 \t NLL Loss: 8.849814\n",
      "Train Epoch: 23 [400/1000 (40%)]\t KLD Loss: 0.063582 \t NLL Loss: 8.608968\n",
      "Train Epoch: 23 [600/1000 (60%)]\t KLD Loss: 0.048852 \t NLL Loss: 9.080752\n",
      "Train Epoch: 23 [800/1000 (80%)]\t KLD Loss: 0.042778 \t NLL Loss: 8.184911\n",
      "====> Epoch: 23 Average loss: 8.5697\n",
      "====> Test set loss: KLD Loss = 0.0477, NLL Loss = 8.3930 \n",
      "Train Epoch: 24 [0/1000 (0%)]\t KLD Loss: 0.067168 \t NLL Loss: 7.963412\n",
      "Train Epoch: 24 [200/1000 (20%)]\t KLD Loss: 0.062784 \t NLL Loss: 8.973556\n",
      "Train Epoch: 24 [400/1000 (40%)]\t KLD Loss: 0.062392 \t NLL Loss: 7.391506\n",
      "Train Epoch: 24 [600/1000 (60%)]\t KLD Loss: 0.054528 \t NLL Loss: 8.729182\n",
      "Train Epoch: 24 [800/1000 (80%)]\t KLD Loss: 0.050842 \t NLL Loss: 7.830907\n",
      "====> Epoch: 24 Average loss: 8.1849\n",
      "====> Test set loss: KLD Loss = 0.0416, NLL Loss = 8.3891 \n",
      "Train Epoch: 25 [0/1000 (0%)]\t KLD Loss: 0.038521 \t NLL Loss: 7.569928\n",
      "Train Epoch: 25 [200/1000 (20%)]\t KLD Loss: 0.050186 \t NLL Loss: 7.563648\n",
      "Train Epoch: 25 [400/1000 (40%)]\t KLD Loss: 0.063939 \t NLL Loss: 8.515490\n",
      "Train Epoch: 25 [600/1000 (60%)]\t KLD Loss: 0.063128 \t NLL Loss: 6.873537\n",
      "Train Epoch: 25 [800/1000 (80%)]\t KLD Loss: 0.033582 \t NLL Loss: 9.985745\n",
      "====> Epoch: 25 Average loss: 8.1649\n",
      "====> Test set loss: KLD Loss = 0.0355, NLL Loss = 8.3979 \n",
      "Train Epoch: 26 [0/1000 (0%)]\t KLD Loss: 0.035697 \t NLL Loss: 7.300902\n",
      "Train Epoch: 26 [200/1000 (20%)]\t KLD Loss: 0.052379 \t NLL Loss: 7.540806\n",
      "Train Epoch: 26 [400/1000 (40%)]\t KLD Loss: 0.043180 \t NLL Loss: 9.222111\n",
      "Train Epoch: 26 [600/1000 (60%)]\t KLD Loss: 0.045783 \t NLL Loss: 8.533763\n",
      "Train Epoch: 26 [800/1000 (80%)]\t KLD Loss: 0.037509 \t NLL Loss: 8.193860\n",
      "====> Epoch: 26 Average loss: 8.1932\n",
      "====> Test set loss: KLD Loss = 0.0376, NLL Loss = 8.3783 \n",
      "Train Epoch: 27 [0/1000 (0%)]\t KLD Loss: 0.050273 \t NLL Loss: 8.328267\n",
      "Train Epoch: 27 [200/1000 (20%)]\t KLD Loss: 0.062004 \t NLL Loss: 7.535089\n",
      "Train Epoch: 27 [400/1000 (40%)]\t KLD Loss: 0.030682 \t NLL Loss: 8.652151\n",
      "Train Epoch: 27 [600/1000 (60%)]\t KLD Loss: 0.035561 \t NLL Loss: 7.650751\n",
      "Train Epoch: 27 [800/1000 (80%)]\t KLD Loss: 0.042132 \t NLL Loss: 8.489566\n",
      "====> Epoch: 27 Average loss: 8.1237\n",
      "====> Test set loss: KLD Loss = 0.0334, NLL Loss = 8.3992 \n",
      "Train Epoch: 28 [0/1000 (0%)]\t KLD Loss: 0.039782 \t NLL Loss: 8.411905\n",
      "Train Epoch: 28 [200/1000 (20%)]\t KLD Loss: 0.041820 \t NLL Loss: 8.091413\n",
      "Train Epoch: 28 [400/1000 (40%)]\t KLD Loss: 0.057028 \t NLL Loss: 7.415573\n",
      "Train Epoch: 28 [600/1000 (60%)]\t KLD Loss: 0.037688 \t NLL Loss: 6.900465\n",
      "Train Epoch: 28 [800/1000 (80%)]\t KLD Loss: 0.047329 \t NLL Loss: 7.567078\n",
      "====> Epoch: 28 Average loss: 8.1182\n",
      "====> Test set loss: KLD Loss = 0.0313, NLL Loss = 8.3760 \n",
      "Train Epoch: 29 [0/1000 (0%)]\t KLD Loss: 0.031997 \t NLL Loss: 8.658608\n",
      "Train Epoch: 29 [200/1000 (20%)]\t KLD Loss: 0.040795 \t NLL Loss: 8.494882\n",
      "Train Epoch: 29 [400/1000 (40%)]\t KLD Loss: 0.039276 \t NLL Loss: 8.405484\n",
      "Train Epoch: 29 [600/1000 (60%)]\t KLD Loss: 0.032564 \t NLL Loss: 8.544681\n",
      "Train Epoch: 29 [800/1000 (80%)]\t KLD Loss: 0.038459 \t NLL Loss: 7.546769\n",
      "====> Epoch: 29 Average loss: 8.0449\n",
      "====> Test set loss: KLD Loss = 0.0356, NLL Loss = 8.2694 \n",
      "Train Epoch: 30 [0/1000 (0%)]\t KLD Loss: 0.039471 \t NLL Loss: 7.070884\n",
      "Train Epoch: 30 [200/1000 (20%)]\t KLD Loss: 0.046913 \t NLL Loss: 7.236992\n",
      "Train Epoch: 30 [400/1000 (40%)]\t KLD Loss: 0.059130 \t NLL Loss: 8.415423\n",
      "Train Epoch: 30 [600/1000 (60%)]\t KLD Loss: 0.044619 \t NLL Loss: 8.280423\n",
      "Train Epoch: 30 [800/1000 (80%)]\t KLD Loss: 0.046352 \t NLL Loss: 7.628691\n",
      "====> Epoch: 30 Average loss: 8.0374\n",
      "====> Test set loss: KLD Loss = 0.0345, NLL Loss = 8.3266 \n",
      "Train Epoch: 31 [0/1000 (0%)]\t KLD Loss: 0.028499 \t NLL Loss: 7.219736\n",
      "Train Epoch: 31 [200/1000 (20%)]\t KLD Loss: 0.048053 \t NLL Loss: 7.644324\n",
      "Train Epoch: 31 [400/1000 (40%)]\t KLD Loss: 0.052081 \t NLL Loss: 6.851751\n",
      "Train Epoch: 31 [600/1000 (60%)]\t KLD Loss: 0.043641 \t NLL Loss: 7.725126\n",
      "Train Epoch: 31 [800/1000 (80%)]\t KLD Loss: 0.047500 \t NLL Loss: 7.166757\n",
      "====> Epoch: 31 Average loss: 7.9678\n",
      "====> Test set loss: KLD Loss = 0.0328, NLL Loss = 8.1981 \n",
      "Train Epoch: 32 [0/1000 (0%)]\t KLD Loss: 0.033296 \t NLL Loss: 7.653018\n",
      "Train Epoch: 32 [200/1000 (20%)]\t KLD Loss: 0.063749 \t NLL Loss: 8.778738\n",
      "Train Epoch: 32 [400/1000 (40%)]\t KLD Loss: 0.043801 \t NLL Loss: 7.305344\n",
      "Train Epoch: 32 [600/1000 (60%)]\t KLD Loss: 0.050535 \t NLL Loss: 8.589326\n",
      "Train Epoch: 32 [800/1000 (80%)]\t KLD Loss: 0.028602 \t NLL Loss: 8.172149\n",
      "====> Epoch: 32 Average loss: 7.9471\n",
      "====> Test set loss: KLD Loss = 0.0301, NLL Loss = 8.1662 \n",
      "Train Epoch: 33 [0/1000 (0%)]\t KLD Loss: 0.035232 \t NLL Loss: 7.652763\n",
      "Train Epoch: 33 [200/1000 (20%)]\t KLD Loss: 0.046261 \t NLL Loss: 7.967070\n",
      "Train Epoch: 33 [400/1000 (40%)]\t KLD Loss: 0.041918 \t NLL Loss: 6.293469\n",
      "Train Epoch: 33 [600/1000 (60%)]\t KLD Loss: 0.050095 \t NLL Loss: 7.057019\n",
      "Train Epoch: 33 [800/1000 (80%)]\t KLD Loss: 0.059164 \t NLL Loss: 8.340472\n",
      "====> Epoch: 33 Average loss: 8.0617\n",
      "====> Test set loss: KLD Loss = 0.0392, NLL Loss = 7.9890 \n",
      "Train Epoch: 34 [0/1000 (0%)]\t KLD Loss: 0.045600 \t NLL Loss: 7.440118\n",
      "Train Epoch: 34 [200/1000 (20%)]\t KLD Loss: 0.059279 \t NLL Loss: 7.260325\n",
      "Train Epoch: 34 [400/1000 (40%)]\t KLD Loss: 0.058119 \t NLL Loss: 7.415079\n",
      "Train Epoch: 34 [600/1000 (60%)]\t KLD Loss: 0.056863 \t NLL Loss: 7.463244\n",
      "Train Epoch: 34 [800/1000 (80%)]\t KLD Loss: 0.049327 \t NLL Loss: 7.232298\n",
      "====> Epoch: 34 Average loss: 7.8493\n",
      "====> Test set loss: KLD Loss = 0.0414, NLL Loss = 7.8291 \n",
      "Train Epoch: 35 [0/1000 (0%)]\t KLD Loss: 0.057337 \t NLL Loss: 5.444990\n",
      "Train Epoch: 35 [200/1000 (20%)]\t KLD Loss: 0.048793 \t NLL Loss: 7.674591\n",
      "Train Epoch: 35 [400/1000 (40%)]\t KLD Loss: 0.050932 \t NLL Loss: 7.386408\n",
      "Train Epoch: 35 [600/1000 (60%)]\t KLD Loss: 0.053876 \t NLL Loss: 6.227194\n",
      "Train Epoch: 35 [800/1000 (80%)]\t KLD Loss: 0.037185 \t NLL Loss: 6.632045\n",
      "====> Epoch: 35 Average loss: 7.7922\n",
      "====> Test set loss: KLD Loss = 0.0519, NLL Loss = 7.5963 \n",
      "End Dimensionality Reduction:::\n",
      "The Median accuracy:::27.208553676202584\n"
     ]
    }
   ],
   "source": [
    "hyper_dim = [35,20]\n",
    "hyper_surrogate = [-1.8259186369806055, 0.6579307791953877]\n",
    "accuracy = hyper_parameters_optimization(hyper_dim,path_latent_train,path_latent_test,hyper_surrogate)\n",
    "print ('The Median accuracy:::'+str(np.median(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on the Possible Values of the Hyper_Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A): Hyper-Parameters for Dimensionality Reduction::::\n",
    "\n",
    "    Autoencoders takes 2 hyper parameters: Number of epochs and batch size respectively, \n",
    "    both of which are discrete variables.\n",
    "    \n",
    "    number of epochs, (Discrete Variable), Possible Values: {10,30,30,...,100}\n",
    "    batch size: (Discrete Variable): Possible Values: {5,10,20,25,50,100}, \n",
    "\n",
    "\"\"\"\n",
    "\"\"\" B): Hyper-Parameters for Polynomials::::\n",
    "\n",
    "    Polynomials takes 2 hyper parameters: alpha and l1_ratio. Notably, both are float values\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html for more details\n",
    "    \n",
    "    Alpha (Continuous Variable, Ranges between -2 and 2, e.g., 0.1234\n",
    "    l1_ratio (Continuous Variable, Ranges between 0 and 1, e.g., 0.97645\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
