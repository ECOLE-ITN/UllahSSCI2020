{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.3.0+cpu cuda: False\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch.utils.data\n",
    "import torch.nn.init as init\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyDOE\n",
    "from scipy import stats\n",
    "import sys\n",
    "import scipy.stats.distributions as dist\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from collections import namedtuple\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "print( 'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dimensionality Reduction Related Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Re-Scale Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Give the path to the training input files as the function argument, \n",
    "the function loads and rescales the initial data set\n",
    "\"\"\"\n",
    "def load_rescale_data_sets(path):\n",
    "    train_data = pd.read_csv(path, index_col=0).iloc[:,:-1]\n",
    "    test_data = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_2_200Samples.csv'), index_col=0).iloc[:,:-1]\n",
    "    cols = test_data.columns\n",
    "    scalar = MinMaxScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(scalar.transform (train_data)) \n",
    "    test_data = pd.DataFrame(scalar.transform (test_data)) \n",
    "    train_data.columns, test_data.columns = cols, cols\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Class Autoencoders, which implements the Autoencoders Neural Networks for Dimensionality Reduction \"\"\"\n",
    "class AE(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_dim, z_dim):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.h1_dim = int (self.x_dim - (1/5) * (self.x_dim - self.z_dim))\n",
    "        self.h2_dim = int (self.x_dim - (2/5) * (self.x_dim - self.z_dim))\n",
    "        self.h3_dim = int (self.x_dim - (3/5) * (self.x_dim - self.z_dim))\n",
    "        self.h4_dim = int (self.x_dim - (4/5) * (self.x_dim - self.z_dim))\n",
    "\n",
    "        #encoder\n",
    "        self.enc = nn.Sequential( nn.Linear(self.x_dim , self.h1_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h1_dim , self.h2_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h2_dim , self.h3_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h3_dim , self.h4_dim), nn.Tanh())\n",
    "        \n",
    "        self.enc_exact = nn.Sequential( nn.Linear(self.h4_dim, self.z_dim) )\n",
    "        \n",
    "        #decoder\n",
    "        \n",
    "        self.dec = nn.Sequential(nn.Linear(self.z_dim , self.h4_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h4_dim , self.h3_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h3_dim , self.h2_dim), nn.Tanh(),\n",
    "                                nn.Linear(self.h2_dim , self.h1_dim), nn.Tanh())\n",
    "                                \n",
    "        self.dec_exact = nn.Sequential( nn.Linear( self.h1_dim, self.x_dim ))\n",
    "\n",
    "\n",
    "    def encode (self, x ):\n",
    "        enc = self.enc(x.float())\n",
    "        enc_exact = self.enc_exact(enc)\n",
    "        return enc_exact\n",
    "\n",
    "\n",
    "    def decode (self, z):\n",
    "        dec = self.dec(z)\n",
    "        dec_exact = self.dec_exact(dec)\n",
    "        return dec_exact\n",
    "\n",
    "    def forward(self, x):\n",
    "        loss = 0\n",
    "        criterion = nn.MSELoss()\n",
    "        #encoder\n",
    "        enc = self.encode(x)\n",
    "        #decoder\n",
    "        dec = self.decode(enc)\n",
    "        loss += criterion(dec, x.float())  \n",
    "        return loss, enc, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions to Implement the Training and Testing of AEs, Based on Methods in Autoencoder Class\"\"\"\n",
    "def train(epoch, train_loader, train_data, batch_size, model):\n",
    "    train_loss = 0\n",
    "    epoch_loss = np.zeros(int(len (train_data) / batch_size ))\n",
    "    clip, learning_rate, seed, print_every, save_every  = 10, 1e-3 , 100, 10, 10\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for batch_idx, (data) in enumerate(train_loader):\n",
    "        \n",
    "        data = Variable(data)\n",
    "        #forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        re_loss,_,_ = model(data)\n",
    "        epoch_loss [batch_idx] = re_loss\n",
    "        loss = re_loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        #printing\n",
    "        if batch_idx % print_every == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] \\t Reconstruction Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                re_loss.data / batch_size))\n",
    "\n",
    "            \n",
    "\n",
    "        train_loss += loss.data\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "    return epoch_loss, model\n",
    "    \n",
    "def test(epoch, test_loader, test_data, model):\n",
    "    \"\"\"uses test data to evaluate \n",
    "    likelihood of the model\"\"\"\n",
    "    mean_re_loss = 0\n",
    "    epoch_loss = np.zeros(len(test_data))\n",
    "    for i, (data) in enumerate(test_loader):                                           \n",
    "        data = Variable(data.reshape(1,-1))\n",
    "        re_loss, _,_ = model(data)\n",
    "        epoch_loss [i] = re_loss\n",
    "        mean_re_loss += re_loss.data\n",
    "\n",
    "    mean_re_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('====> Test set loss: Reconstruction Loss = {:.4f} '.format(\n",
    "        mean_re_loss))\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is the method that implements the dimensionality reduction based on Autoencoders \"\"\"\n",
    "def perform_dimensionality_reduction (hyper_paras, path):\n",
    "    seed = 100\n",
    "    z_dim = int(50-0.9 * 50) # 90 % dimensionality reduction, Change it based on the Value of L in the paper\n",
    "    n_epochs, batch_size = hyper_paras\n",
    "    train_data, test_data = load_rescale_data_sets(path)\n",
    "    x_dim = train_data.shape[1]\n",
    "    \n",
    "    h1_dim = int (x_dim - (1/5) * (x_dim - z_dim))\n",
    "    h2_dim = int (x_dim - (2/5) * (x_dim - z_dim))\n",
    "    h3_dim = int (x_dim - (3/5) * (x_dim - z_dim))\n",
    "    h4_dim = int (x_dim - (4/5) * (x_dim - z_dim))\n",
    "    \n",
    "    print (x_dim, h1_dim, h2_dim, h3_dim,h4_dim,z_dim)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    train_loader = torch.utils.data.DataLoader ( dataset = train_data.values ,  batch_size = batch_size , shuffle= True)\n",
    "    test_loader = torch.utils.data.DataLoader (  dataset = test_data.values , shuffle= True)\n",
    "    train_error = np.zeros([n_epochs , int(train_data.shape[0] / batch_size ) ])\n",
    "    test_error = np.zeros([n_epochs , test_data.shape[0]])  \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        #training + testing\n",
    "        if (epoch==1): ## only for the first time, take the default model, all the next times in iteration, keep improving it\n",
    "            model = AE(x_dim, z_dim)\n",
    "            \n",
    "        tr = train(epoch, train_loader, train_data, batch_size, model)\n",
    "        train_error [epoch-1 , :] = tr [0]\n",
    "        model = tr[1]\n",
    "        te = test(epoch, test_loader, test_data, model)\n",
    "        test_error [epoch-1 , :] = te [0]\n",
    "            \n",
    "    train_lat = [ model (Variable(torch.tensor(train_data.iloc[idx,:].values)).reshape(1,-1))[1] for idx in range(len(train_data)) ]\n",
    "    test_lat = [ model (Variable(torch.tensor(test_data.iloc[idx,:].values)).reshape(1,-1))[1] for idx in range(len(test_data)) ]\n",
    "    train_lat = pd.DataFrame(torch.cat(train_lat).cpu().detach().numpy())\n",
    "    test_lat = pd.DataFrame(torch.cat(test_lat).cpu().detach().numpy())\n",
    "    cols = []\n",
    "    for i in range(train_lat.shape[1]):\n",
    "        cols.append(str('Z'+str(i+1)))\n",
    "    train_lat.columns = cols\n",
    "    test_lat.columns = cols\n",
    "    train_lat.to_csv('AEs_Kriging_50D_90%_latent_training.csv')  # Save the Latent Training Representation \n",
    "    test_lat.to_csv('AEs_Kriging_50D_90%_latent_test.csv') # Save the Latent Test Representation \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Dimensionality Reduction Related Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New Reduced Data Sets for all Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_f2(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_2_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f3(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_3_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f7(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_7_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f9(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_9_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f10(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_10_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f13(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_13_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f15(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_15_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f16(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_16_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f20(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_20_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f24(path,path_latent_train,path_latent_test):\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_24_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Start Kriging Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Kriging'''\n",
    "def kriging(train_data,test_data):\n",
    "    kernel =  RationalQuadratic()\n",
    "    scaler = MinMaxScaler().fit(np.r_[train_data.iloc[:,:-1].values])\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel,random_state=0,\n",
    "                                   normalize_y=True ).fit(scaler.transform(train_data.iloc[:,:-1]), train_data.iloc[:,-1])\n",
    "    pred = gpr.predict(scaler.transform(test_data))\n",
    "    return gpr,pred\n",
    "\n",
    "\"\"\" Normalized Mean Absolute Error % \"\"\"\n",
    "def rmae(true, pred):\n",
    "    return np.mean((abs(true-pred) / abs(true)) * 100),pred\n",
    "\n",
    "\"\"\" This method implements and evaluates the Kriging Surrogate Model with RMAE \"\"\"\n",
    "def surrogate_model(train_data,test_data,true):\n",
    "    kri_model, kri_pred = kriging(train_data,test_data.iloc[:,:-1])\n",
    "    return rmae(true,kri_pred)\n",
    "\n",
    "\"\"\" Implements all the surrogate models, i.e., for all test function, and returns the median of RMAE errors,\n",
    "This median is used as the primary metric for Hyper-Parameters Optimization \"\"\"\n",
    "def perform_surrogate_modeling(paths, path_latent_train,path_latent_test):\n",
    "    train_2, test_2, true_2 = load_f2(paths[0],path_latent_train,path_latent_test)\n",
    "    rmae_2,pred_2 = surrogate_model(train_2, test_2, true_2)\n",
    "    \n",
    "    train_3, test_3, true_3 = load_f3(paths[1],path_latent_train,path_latent_test)\n",
    "    rmae_3,pred_3 = surrogate_model(train_3, test_3, true_3)\n",
    "    \n",
    "    train_7, test_7, true_7 = load_f7(paths[2],path_latent_train,path_latent_test)\n",
    "    rmae_7,pred_7 = surrogate_model(train_7, test_7, true_7)\n",
    "    \n",
    "    train_9, test_9, true_9 = load_f9(paths[3],path_latent_train,path_latent_test)\n",
    "    rmae_9,pred_9 = surrogate_model(train_9, test_9, true_9)\n",
    "    \n",
    "    train_10, test_10, true_10 = load_f10(paths[4],path_latent_train,path_latent_test)\n",
    "    rmae_10,pred_10 = surrogate_model(train_10, test_10, true_10)\n",
    "    \n",
    "    train_13, test_13, true_13 = load_f13(paths[5],path_latent_train,path_latent_test)\n",
    "    rmae_13,pred_13 = surrogate_model(train_13, test_13, true_13)\n",
    "    \n",
    "    train_15, test_15, true_15 = load_f15(paths[6],path_latent_train,path_latent_test)\n",
    "    rmae_15,pred_15 = surrogate_model(train_15, test_15, true_15)\n",
    "    \n",
    "    train_16, test_16, true_16 = load_f16(paths[7],path_latent_train,path_latent_test)\n",
    "    rmae_16,pred_16 = surrogate_model(train_16, test_16, true_16)\n",
    "    \n",
    "    train_20, test_20, true_20 = load_f20(paths[8],path_latent_train,path_latent_test)\n",
    "    rmae_20,pred_20 = surrogate_model(train_20, test_20, true_20)\n",
    "    \n",
    "    train_24, test_24, true_24 = load_f24(paths[9],path_latent_train,path_latent_test)\n",
    "    rmae_24,pred_24 = surrogate_model(train_24, test_24, true_24)\n",
    "    \n",
    "    accuracy = [rmae_2,rmae_3,rmae_7,rmae_9,rmae_10,rmae_13,rmae_15,rmae_16,rmae_20,rmae_24]\n",
    "    pred = [pred_2,pred_3,pred_7,pred_9,pred_10,pred_13,pred_15,pred_16,pred_20,pred_24]\n",
    "    return accuracy,pred\n",
    "\n",
    "\"\"\" This is the function used for Hyper_Parameters_Optimization for both dimensionality reduction and surrogate modelling \"\"\"\n",
    "def hyper_parameters_optimization(hyper_dim,path_latent_train,path_latent_test):\n",
    "    print ('Start Dimensionality Reduction:::')\n",
    "    _ , _ = perform_dimensionality_reduction (hyper_dim, paths[0])\n",
    "    print ('End Dimensionality Reduction:::')\n",
    "    accuracy,pred = perform_surrogate_modeling (paths,path_latent_train,path_latent_test)\n",
    "    return accuracy,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \n",
    "path_2 = \"Data Generation/50 D/Training_Data_Sets/train_2_1000Samples.csv\"\n",
    "path_3 = \"Data Generation/50 D/Training_Data_Sets/train_3_1000Samples.csv\"\n",
    "path_7 = \"Data Generation/50 D/Training_Data_Sets/train_7_1000Samples.csv\"\n",
    "path_9 = \"Data Generation/50 D/Training_Data_Sets/train_9_1000Samples.csv\"\n",
    "path_10 = \"Data Generation/50 D/Training_Data_Sets/train_10_1000Samples.csv\"\n",
    "path_13 = \"Data Generation/50 D/Training_Data_Sets/train_13_1000Samples.csv\"\n",
    "path_15 = \"Data Generation/50 D/Training_Data_Sets/train_15_1000Samples.csv\"\n",
    "path_16 = \"Data Generation/50 D/Training_Data_Sets/train_16_1000Samples.csv\"\n",
    "path_20 = \"Data Generation/50 D/Training_Data_Sets/train_201000Samples.csv\"\n",
    "path_24 = \"Data Generation/50 D/Training_Data_Sets/train_241000Samples.csv\"\n",
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \n",
    "path_latent_train = \"AEs_Kriging_50D_90%_latent_training.csv\"\n",
    "path_latent_test = \"AEs_Kriging_50D_90%_latent_test.csv\"\n",
    "paths = [path_2,path_3,path_7,path_9,path_10,path_13,path_15,path_16,path_20,path_24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Dimensionality Reduction:::\n",
      "50 41 32 23 14 5\n",
      "Train Epoch: 1 [0/1000 (0%)] \t Reconstruction Loss: 0.011650\n",
      "Train Epoch: 1 [250/1000 (25%)] \t Reconstruction Loss: 0.009038\n",
      "Train Epoch: 1 [500/1000 (50%)] \t Reconstruction Loss: 0.004823\n",
      "Train Epoch: 1 [750/1000 (75%)] \t Reconstruction Loss: 0.003900\n",
      "====> Epoch: 1 Average loss: 0.0065\n",
      "====> Test set loss: Reconstruction Loss = 0.0864 \n",
      "Train Epoch: 2 [0/1000 (0%)] \t Reconstruction Loss: 0.003503\n",
      "Train Epoch: 2 [250/1000 (25%)] \t Reconstruction Loss: 0.003291\n",
      "Train Epoch: 2 [500/1000 (50%)] \t Reconstruction Loss: 0.003375\n",
      "Train Epoch: 2 [750/1000 (75%)] \t Reconstruction Loss: 0.003387\n",
      "====> Epoch: 2 Average loss: 0.0034\n",
      "====> Test set loss: Reconstruction Loss = 0.0846 \n",
      "Train Epoch: 3 [0/1000 (0%)] \t Reconstruction Loss: 0.003280\n",
      "Train Epoch: 3 [250/1000 (25%)] \t Reconstruction Loss: 0.003265\n",
      "Train Epoch: 3 [500/1000 (50%)] \t Reconstruction Loss: 0.003441\n",
      "Train Epoch: 3 [750/1000 (75%)] \t Reconstruction Loss: 0.003377\n",
      "====> Epoch: 3 Average loss: 0.0034\n",
      "====> Test set loss: Reconstruction Loss = 0.0842 \n",
      "Train Epoch: 4 [0/1000 (0%)] \t Reconstruction Loss: 0.003564\n",
      "Train Epoch: 4 [250/1000 (25%)] \t Reconstruction Loss: 0.003295\n",
      "Train Epoch: 4 [500/1000 (50%)] \t Reconstruction Loss: 0.003325\n",
      "Train Epoch: 4 [750/1000 (75%)] \t Reconstruction Loss: 0.003392\n",
      "====> Epoch: 4 Average loss: 0.0034\n",
      "====> Test set loss: Reconstruction Loss = 0.0841 \n",
      "Train Epoch: 5 [0/1000 (0%)] \t Reconstruction Loss: 0.003405\n",
      "Train Epoch: 5 [250/1000 (25%)] \t Reconstruction Loss: 0.003331\n",
      "Train Epoch: 5 [500/1000 (50%)] \t Reconstruction Loss: 0.003325\n",
      "Train Epoch: 5 [750/1000 (75%)] \t Reconstruction Loss: 0.003232\n",
      "====> Epoch: 5 Average loss: 0.0033\n",
      "====> Test set loss: Reconstruction Loss = 0.0833 \n",
      "Train Epoch: 6 [0/1000 (0%)] \t Reconstruction Loss: 0.003311\n",
      "Train Epoch: 6 [250/1000 (25%)] \t Reconstruction Loss: 0.003302\n",
      "Train Epoch: 6 [500/1000 (50%)] \t Reconstruction Loss: 0.003430\n",
      "Train Epoch: 6 [750/1000 (75%)] \t Reconstruction Loss: 0.003266\n",
      "====> Epoch: 6 Average loss: 0.0033\n",
      "====> Test set loss: Reconstruction Loss = 0.0822 \n",
      "Train Epoch: 7 [0/1000 (0%)] \t Reconstruction Loss: 0.003249\n",
      "Train Epoch: 7 [250/1000 (25%)] \t Reconstruction Loss: 0.003168\n",
      "Train Epoch: 7 [500/1000 (50%)] \t Reconstruction Loss: 0.003120\n",
      "Train Epoch: 7 [750/1000 (75%)] \t Reconstruction Loss: 0.003259\n",
      "====> Epoch: 7 Average loss: 0.0033\n",
      "====> Test set loss: Reconstruction Loss = 0.0816 \n",
      "Train Epoch: 8 [0/1000 (0%)] \t Reconstruction Loss: 0.003244\n",
      "Train Epoch: 8 [250/1000 (25%)] \t Reconstruction Loss: 0.003177\n",
      "Train Epoch: 8 [500/1000 (50%)] \t Reconstruction Loss: 0.003185\n",
      "Train Epoch: 8 [750/1000 (75%)] \t Reconstruction Loss: 0.003275\n",
      "====> Epoch: 8 Average loss: 0.0032\n",
      "====> Test set loss: Reconstruction Loss = 0.0808 \n",
      "Train Epoch: 9 [0/1000 (0%)] \t Reconstruction Loss: 0.003108\n",
      "Train Epoch: 9 [250/1000 (25%)] \t Reconstruction Loss: 0.003090\n",
      "Train Epoch: 9 [500/1000 (50%)] \t Reconstruction Loss: 0.003296\n",
      "Train Epoch: 9 [750/1000 (75%)] \t Reconstruction Loss: 0.003160\n",
      "====> Epoch: 9 Average loss: 0.0032\n",
      "====> Test set loss: Reconstruction Loss = 0.0799 \n",
      "Train Epoch: 10 [0/1000 (0%)] \t Reconstruction Loss: 0.003092\n",
      "Train Epoch: 10 [250/1000 (25%)] \t Reconstruction Loss: 0.003312\n",
      "Train Epoch: 10 [500/1000 (50%)] \t Reconstruction Loss: 0.003119\n",
      "Train Epoch: 10 [750/1000 (75%)] \t Reconstruction Loss: 0.003051\n",
      "====> Epoch: 10 Average loss: 0.0031\n",
      "====> Test set loss: Reconstruction Loss = 0.0796 \n",
      "Train Epoch: 11 [0/1000 (0%)] \t Reconstruction Loss: 0.003038\n",
      "Train Epoch: 11 [250/1000 (25%)] \t Reconstruction Loss: 0.003182\n",
      "Train Epoch: 11 [500/1000 (50%)] \t Reconstruction Loss: 0.003071\n",
      "Train Epoch: 11 [750/1000 (75%)] \t Reconstruction Loss: 0.003291\n",
      "====> Epoch: 11 Average loss: 0.0031\n",
      "====> Test set loss: Reconstruction Loss = 0.0794 \n",
      "Train Epoch: 12 [0/1000 (0%)] \t Reconstruction Loss: 0.003283\n",
      "Train Epoch: 12 [250/1000 (25%)] \t Reconstruction Loss: 0.003001\n",
      "Train Epoch: 12 [500/1000 (50%)] \t Reconstruction Loss: 0.003252\n",
      "Train Epoch: 12 [750/1000 (75%)] \t Reconstruction Loss: 0.003113\n",
      "====> Epoch: 12 Average loss: 0.0031\n",
      "====> Test set loss: Reconstruction Loss = 0.0789 \n",
      "Train Epoch: 13 [0/1000 (0%)] \t Reconstruction Loss: 0.003031\n",
      "Train Epoch: 13 [250/1000 (25%)] \t Reconstruction Loss: 0.003156\n",
      "Train Epoch: 13 [500/1000 (50%)] \t Reconstruction Loss: 0.003130\n",
      "Train Epoch: 13 [750/1000 (75%)] \t Reconstruction Loss: 0.002977\n",
      "====> Epoch: 13 Average loss: 0.0031\n",
      "====> Test set loss: Reconstruction Loss = 0.0779 \n",
      "Train Epoch: 14 [0/1000 (0%)] \t Reconstruction Loss: 0.003077\n",
      "Train Epoch: 14 [250/1000 (25%)] \t Reconstruction Loss: 0.003123\n",
      "Train Epoch: 14 [500/1000 (50%)] \t Reconstruction Loss: 0.003048\n",
      "Train Epoch: 14 [750/1000 (75%)] \t Reconstruction Loss: 0.003171\n",
      "====> Epoch: 14 Average loss: 0.0031\n",
      "====> Test set loss: Reconstruction Loss = 0.0775 \n",
      "Train Epoch: 15 [0/1000 (0%)] \t Reconstruction Loss: 0.003077\n",
      "Train Epoch: 15 [250/1000 (25%)] \t Reconstruction Loss: 0.003068\n",
      "Train Epoch: 15 [500/1000 (50%)] \t Reconstruction Loss: 0.003010\n",
      "Train Epoch: 15 [750/1000 (75%)] \t Reconstruction Loss: 0.003036\n",
      "====> Epoch: 15 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0771 \n",
      "Train Epoch: 16 [0/1000 (0%)] \t Reconstruction Loss: 0.002841\n",
      "Train Epoch: 16 [250/1000 (25%)] \t Reconstruction Loss: 0.002937\n",
      "Train Epoch: 16 [500/1000 (50%)] \t Reconstruction Loss: 0.003155\n",
      "Train Epoch: 16 [750/1000 (75%)] \t Reconstruction Loss: 0.003081\n",
      "====> Epoch: 16 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0767 \n",
      "Train Epoch: 17 [0/1000 (0%)] \t Reconstruction Loss: 0.002973\n",
      "Train Epoch: 17 [250/1000 (25%)] \t Reconstruction Loss: 0.003041\n",
      "Train Epoch: 17 [500/1000 (50%)] \t Reconstruction Loss: 0.003022\n",
      "Train Epoch: 17 [750/1000 (75%)] \t Reconstruction Loss: 0.002933\n",
      "====> Epoch: 17 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 18 [0/1000 (0%)] \t Reconstruction Loss: 0.002803\n",
      "Train Epoch: 18 [250/1000 (25%)] \t Reconstruction Loss: 0.002869\n",
      "Train Epoch: 18 [500/1000 (50%)] \t Reconstruction Loss: 0.002964\n",
      "Train Epoch: 18 [750/1000 (75%)] \t Reconstruction Loss: 0.002943\n",
      "====> Epoch: 18 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0764 \n",
      "Train Epoch: 19 [0/1000 (0%)] \t Reconstruction Loss: 0.003023\n",
      "Train Epoch: 19 [250/1000 (25%)] \t Reconstruction Loss: 0.002986\n",
      "Train Epoch: 19 [500/1000 (50%)] \t Reconstruction Loss: 0.002973\n",
      "Train Epoch: 19 [750/1000 (75%)] \t Reconstruction Loss: 0.002945\n",
      "====> Epoch: 19 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 20 [0/1000 (0%)] \t Reconstruction Loss: 0.003033\n",
      "Train Epoch: 20 [250/1000 (25%)] \t Reconstruction Loss: 0.003020\n",
      "Train Epoch: 20 [500/1000 (50%)] \t Reconstruction Loss: 0.002935\n",
      "Train Epoch: 20 [750/1000 (75%)] \t Reconstruction Loss: 0.002903\n",
      "====> Epoch: 20 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 21 [0/1000 (0%)] \t Reconstruction Loss: 0.002862\n",
      "Train Epoch: 21 [250/1000 (25%)] \t Reconstruction Loss: 0.002945\n",
      "Train Epoch: 21 [500/1000 (50%)] \t Reconstruction Loss: 0.003013\n",
      "Train Epoch: 21 [750/1000 (75%)] \t Reconstruction Loss: 0.002919\n",
      "====> Epoch: 21 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 22 [0/1000 (0%)] \t Reconstruction Loss: 0.002859\n",
      "Train Epoch: 22 [250/1000 (25%)] \t Reconstruction Loss: 0.002891\n",
      "Train Epoch: 22 [500/1000 (50%)] \t Reconstruction Loss: 0.002932\n",
      "Train Epoch: 22 [750/1000 (75%)] \t Reconstruction Loss: 0.002992\n",
      "====> Epoch: 22 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 23 [0/1000 (0%)] \t Reconstruction Loss: 0.002925\n",
      "Train Epoch: 23 [250/1000 (25%)] \t Reconstruction Loss: 0.003005\n",
      "Train Epoch: 23 [500/1000 (50%)] \t Reconstruction Loss: 0.002901\n",
      "Train Epoch: 23 [750/1000 (75%)] \t Reconstruction Loss: 0.003018\n",
      "====> Epoch: 23 Average loss: 0.0030\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 24 [0/1000 (0%)] \t Reconstruction Loss: 0.002840\n",
      "Train Epoch: 24 [250/1000 (25%)] \t Reconstruction Loss: 0.003001\n",
      "Train Epoch: 24 [500/1000 (50%)] \t Reconstruction Loss: 0.002902\n",
      "Train Epoch: 24 [750/1000 (75%)] \t Reconstruction Loss: 0.002747\n",
      "====> Epoch: 24 Average loss: 0.0029\n",
      "====> Test set loss: Reconstruction Loss = 0.0764 \n",
      "Train Epoch: 25 [0/1000 (0%)] \t Reconstruction Loss: 0.003034\n",
      "Train Epoch: 25 [250/1000 (25%)] \t Reconstruction Loss: 0.002831\n",
      "Train Epoch: 25 [500/1000 (50%)] \t Reconstruction Loss: 0.002836\n",
      "Train Epoch: 25 [750/1000 (75%)] \t Reconstruction Loss: 0.002765\n",
      "====> Epoch: 25 Average loss: 0.0029\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 26 [0/1000 (0%)] \t Reconstruction Loss: 0.002910\n",
      "Train Epoch: 26 [250/1000 (25%)] \t Reconstruction Loss: 0.002926\n",
      "Train Epoch: 26 [500/1000 (50%)] \t Reconstruction Loss: 0.002851\n",
      "Train Epoch: 26 [750/1000 (75%)] \t Reconstruction Loss: 0.002917\n",
      "====> Epoch: 26 Average loss: 0.0029\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 27 [0/1000 (0%)] \t Reconstruction Loss: 0.002982\n",
      "Train Epoch: 27 [250/1000 (25%)] \t Reconstruction Loss: 0.002888\n",
      "Train Epoch: 27 [500/1000 (50%)] \t Reconstruction Loss: 0.003055\n",
      "Train Epoch: 27 [750/1000 (75%)] \t Reconstruction Loss: 0.002936\n",
      "====> Epoch: 27 Average loss: 0.0029\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 28 [0/1000 (0%)] \t Reconstruction Loss: 0.002695\n",
      "Train Epoch: 28 [250/1000 (25%)] \t Reconstruction Loss: 0.002907\n",
      "Train Epoch: 28 [500/1000 (50%)] \t Reconstruction Loss: 0.002951\n",
      "Train Epoch: 28 [750/1000 (75%)] \t Reconstruction Loss: 0.002952\n",
      "====> Epoch: 28 Average loss: 0.0029\n",
      "====> Test set loss: Reconstruction Loss = 0.0762 \n",
      "Train Epoch: 29 [0/1000 (0%)] \t Reconstruction Loss: 0.002810\n",
      "Train Epoch: 29 [250/1000 (25%)] \t Reconstruction Loss: 0.002873\n",
      "Train Epoch: 29 [500/1000 (50%)] \t Reconstruction Loss: 0.002976\n",
      "Train Epoch: 29 [750/1000 (75%)] \t Reconstruction Loss: 0.002844\n",
      "====> Epoch: 29 Average loss: 0.0029\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "Train Epoch: 30 [0/1000 (0%)] \t Reconstruction Loss: 0.002927\n",
      "Train Epoch: 30 [250/1000 (25%)] \t Reconstruction Loss: 0.002877\n",
      "Train Epoch: 30 [500/1000 (50%)] \t Reconstruction Loss: 0.003105\n",
      "Train Epoch: 30 [750/1000 (75%)] \t Reconstruction Loss: 0.002954\n",
      "====> Epoch: 30 Average loss: 0.0029\n",
      "====> Test set loss: Reconstruction Loss = 0.0763 \n",
      "End Dimensionality Reduction:::\n",
      "The Mean accuracy:::26.19449672652313\n"
     ]
    }
   ],
   "source": [
    "hyper_dim = [30,25]\n",
    "accuracy,pred = hyper_parameters_optimization(hyper_dim,path_latent_train,path_latent_test)\n",
    "print ('The Mean accuracy:::'+str(np.mean(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on the Possible Values of the Hyper_Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyper-Parameters for Dimensionality Reduction::::\n",
    "\n",
    "    Autoencoders takes 2 hyper parameters: number of epochs and batch size respectively, \n",
    "    both of which are discrete variables. Below are the possible candidates. \n",
    "    \n",
    "    number of epochs, (Discrete Variable), Possible Values: {10,20,30,...,100} # increase of 10 every time\n",
    "    batch size: (Discrete Variable): Possible Values: {5,10,20,25,50,100} # Hint: any value that divides 1000 perfectly\n",
    "\"\"\"\n",
    "pd.DataFrame(np.array(accuracy), columns = ['Accuracy']).to_csv('Accuracy.csv')\n",
    "model = AE(50, 5)\n",
    "array = np.zeros((10000,5))\n",
    "for i in range(len(array)):\n",
    "    array [i] = model(Variable(torch.tensor(np.zeros(50))))[1].cpu().detach().numpy()\n",
    "array1 = np.zeros((10000,5))\n",
    "for i in range(len(array)):\n",
    "    array1 [i] = model(Variable(torch.tensor(np.ones(50))))[1].cpu().detach().numpy()\n",
    "np.mean(abs(np.mean(array1, 0)-np.mean(array, 0)))/4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
