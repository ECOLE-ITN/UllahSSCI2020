{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyDOE\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "from collections import namedtuple\n",
    "import cma\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.stats.distributions as dist\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import bbobbenchmarks as bn\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New Reduced Data Sets for all Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_f2(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f2.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f2.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_2_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f3(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f3.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f3.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_3_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f7(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f7.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f7.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_7_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f9(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f9.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f9.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_9_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f10(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f10.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f10.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_10_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f13(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f13.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f13.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_13_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f15(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f15.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f15.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_15_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f16(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f16.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f16.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_16_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f20(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f20.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f20.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_20_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f24(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_ELN_50D_30%_latent_training_f24.csv\"\n",
    "    path_latent_test =  \"KPCA_ELN_50D_30%_latent_test_f24.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_24_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Models & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueRange = namedtuple('ValueRange', ['min', 'max'])\n",
    "\n",
    "def determinerange(values):\n",
    "    \"\"\"Determine the range of values in each dimension\"\"\"\n",
    "    return ValueRange(np.min(values, axis=0), np.max(values, axis=0))\n",
    "\n",
    "\n",
    "def linearscaletransform(values, *, range_in=None, range_out=ValueRange(0, 1), scale_only=False):\n",
    "    \"\"\"Perform a scale transformation of `values`: [range_in] --> [range_out]\"\"\"\n",
    "\n",
    "    if range_in is None:\n",
    "        range_in = determinerange(values)\n",
    "    elif not isinstance(range_in, ValueRange):\n",
    "        range_in = ValueRange(*range_in)\n",
    "\n",
    "    if not isinstance(range_out, ValueRange):\n",
    "        range_out = ValueRange(*range_out)\n",
    "\n",
    "    scale_out = range_out.max - range_out.min\n",
    "    scale_in = range_in.max - range_in.min\n",
    "\n",
    "    if scale_only:\n",
    "        scaled_values = (values / scale_in) * scale_out\n",
    "    else:\n",
    "        scaled_values = (values - range_in.min) / scale_in\n",
    "        scaled_values = (scaled_values * scale_out) + range_out.min\n",
    "\n",
    "    return scaled_values\n",
    "\n",
    "\n",
    "''' F2 '''\n",
    "def F2(X):\n",
    "    f = bn.F2()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F3 '''\n",
    "def F3(X):\n",
    "    f = bn.F3()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F7 '''\n",
    "def F7(X):\n",
    "    f = bn.F7()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F9 '''\n",
    "def F9(X):\n",
    "    f = bn.F9()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F10 '''\n",
    "def F10(X):\n",
    "    f = bn.F10()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F13 '''\n",
    "def F13(X):\n",
    "    f = bn.F13()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F15 '''\n",
    "def F15(X):\n",
    "    f = bn.F15()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F16 '''\n",
    "def F16(X):\n",
    "    f = bn.F16()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F20 '''\n",
    "def F20(X):\n",
    "    f = bn.F20()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' F24 '''\n",
    "def F24(X):\n",
    "    f = bn.F24()\n",
    "    X = np.array(X)\n",
    "    return f(X)\n",
    "\n",
    "''' Latin HyperCube Sampling Design of Experiment '''\n",
    "def DOE(n_obs, dim):\n",
    "    np.random.seed(0)\n",
    "    lhd = pyDOE.lhs(n=dim, samples=n_obs, criterion='m')\n",
    "    X = [lhd[:,idx] for idx in range(dim)]\n",
    "    return X\n",
    "\n",
    "def create_basis_function(data):\n",
    "    true = np.array(data['Y'])\n",
    "    data = pd.DataFrame(np.atleast_2d(PolynomialFeatures(degree=2).fit_transform(data.iloc[:,:-1])))\n",
    "    data['Y'] = pd.Series(true)\n",
    "    return data\n",
    "\n",
    "''' Create Basis Functions '''\n",
    "def create_function_basis(x):\n",
    "    return np.atleast_2d(PolynomialFeatures(degree=2).fit_transform(x.reshape(1,-1)))\n",
    "      \n",
    "\n",
    "''' Elastic Net Regression '''\n",
    "def elastic_net(train_data,test_data,hyper_surrogate):\n",
    "    alp, rat = hyper_surrogate\n",
    "    scaler =  MinMaxScaler().fit(np.r_[train_data.iloc[:,:-1].values])\n",
    "    regr = ElasticNet(alpha= np.power(10,alp) ,random_state=0 , l1_ratio=rat, fit_intercept =True, max_iter=3000,selection='random').fit(scaler.transform ( np.array(train_data.iloc[:,:-1])) ,  np.array(train_data.iloc[:,-1]))\n",
    "    pred = regr.predict(scaler.transform(test_data))\n",
    "    def predict(scaler, regr):\n",
    "        def __predict__(x):\n",
    "            x = create_function_basis(x)\n",
    "            return regr.predict(scaler.transform(x))\n",
    "        return __predict__\n",
    "    return regr,pred, predict(scaler, regr)\n",
    "\n",
    "\"\"\" This method implements and evaluates the Kriging Surrogate Model with RMAE \"\"\"\n",
    "def surrogate_model(train_data,test_data,hyper_surrogate):\n",
    "    train_data = create_basis_function(train_data)\n",
    "    test_data = create_basis_function(test_data)\n",
    "    model_eln , pred_eln , predict_eln = elastic_net(train_data,test_data.iloc[:,:-1],hyper_surrogate)\n",
    "    return predict_eln\n",
    "\n",
    "\"\"\" Implements all the surrogate models, i.e., for all test function, and returns the median of RMAE errors,\n",
    "This median is used as the primary metric for Hyper-Parameters Optimization \"\"\"\n",
    "def perform_surrogate_modeling(paths,hyper_surrogate):\n",
    "    train_2, test_2, _ = load_f2(paths[0])\n",
    "    predict_eln_2 = surrogate_model(train_2, test_2,hyper_surrogate)\n",
    "    \n",
    "    train_3, test_3, _ = load_f3(paths[1])\n",
    "    predict_eln_3 = surrogate_model(train_3, test_3,hyper_surrogate)\n",
    "    \n",
    "    train_7, test_7, _ = load_f7(paths[2])\n",
    "    predict_eln_7 = surrogate_model(train_7, test_7,hyper_surrogate)\n",
    "    \n",
    "    train_9, test_9, _ = load_f9(paths[3])\n",
    "    predict_eln_9 = surrogate_model(train_9, test_9,hyper_surrogate)\n",
    "    \n",
    "    train_10, test_10, _ = load_f10(paths[4])\n",
    "    predict_eln_10 = surrogate_model(train_10, test_10,hyper_surrogate)\n",
    "    \n",
    "    train_13, test_13, _ = load_f13(paths[5])\n",
    "    predict_eln_13 = surrogate_model(train_13, test_13,hyper_surrogate)\n",
    "    \n",
    "    train_15, test_15, _ = load_f15(paths[6])\n",
    "    predict_eln_15 = surrogate_model(train_15, test_15,hyper_surrogate)\n",
    "    \n",
    "    train_16, test_16, _ = load_f16(paths[7])\n",
    "    predict_eln_16 = surrogate_model(train_16, test_16,hyper_surrogate)\n",
    "    \n",
    "    train_20, test_20, _ = load_f20(paths[8])\n",
    "    predict_eln_20 = surrogate_model(train_20, test_20,hyper_surrogate)\n",
    "    \n",
    "    train_24, test_24, _ = load_f24(paths[9])\n",
    "    predict_eln_24 = surrogate_model(train_24, test_24,hyper_surrogate)\n",
    "    \n",
    "    eln = [predict_eln_2,predict_eln_3,predict_eln_7,predict_eln_9,predict_eln_10,predict_eln_13,\n",
    "          predict_eln_15,predict_eln_16,predict_eln_20,predict_eln_24]\n",
    "    \n",
    "    return eln\n",
    "\"\"\" This is the function used for Hyper_Parameters_Optimization for both dimensionality reduction and surrogate modelling \"\"\"\n",
    "def run(paths,hyper_surrogate):\n",
    "    kri = perform_surrogate_modeling (paths,hyper_surrogate)\n",
    "    return kri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \n",
    "path_2 = \"Data Generation/50 D/Training_Data_Sets/train_2_1000Samples.csv\"\n",
    "path_3 = \"Data Generation/50 D/Training_Data_Sets/train_3_1000Samples.csv\"\n",
    "path_7 = \"Data Generation/50 D/Training_Data_Sets/train_7_1000Samples.csv\"\n",
    "path_9 = \"Data Generation/50 D/Training_Data_Sets/train_9_1000Samples.csv\"\n",
    "path_10 = \"Data Generation/50 D/Training_Data_Sets/train_10_1000Samples.csv\"\n",
    "path_13 = \"Data Generation/50 D/Training_Data_Sets/train_13_1000Samples.csv\"\n",
    "path_15 = \"Data Generation/50 D/Training_Data_Sets/train_15_1000Samples.csv\"\n",
    "path_16 = \"Data Generation/50 D/Training_Data_Sets/train_16_1000Samples.csv\"\n",
    "path_20 = \"Data Generation/50 D/Training_Data_Sets/train_201000Samples.csv\"\n",
    "path_24 = \"Data Generation/50 D/Training_Data_Sets/train_241000Samples.csv\"\n",
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \n",
    "paths = [path_2,path_3,path_7,path_9,path_10,path_13,path_15,path_16,path_20,path_24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Dimensionality Reduction and Construct the Surrogate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_surrogate = [-1.9994031989846746, 0.7809306886794394]\n",
    "eln = run(paths,hyper_surrogate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = int(50 - (0.3 * 50)) # change here\n",
    "Columns = ['ELN']\n",
    "Cols = []\n",
    "for j in range(len(Columns)):\n",
    "    for i in range(1,dimensionality+1):\n",
    "        Cols.append(Columns[j]+'_Z'+str(i))\n",
    "opt = {'maxfun':1000 * dimensionality,'ftol':0.0}\n",
    "const = Bounds([-5] * dimensionality, [5] * dimensionality)        \n",
    "n_obs =  30\n",
    "G = DOE(n_obs, dimensionality)\n",
    "G = [ G[idx].reshape(n_obs,1) for idx in range(len(G)) ]\n",
    "X_2_Values = np.zeros([n_obs,dimensionality])\n",
    "X_3_Values = np.zeros([n_obs,dimensionality])\n",
    "X_7_Values = np.zeros([n_obs,dimensionality])\n",
    "X_9_Values = np.zeros([n_obs,dimensionality])\n",
    "X_10_Values = np.zeros([n_obs,dimensionality])\n",
    "X_13_Values = np.zeros([n_obs,dimensionality])\n",
    "X_15_Values = np.zeros([n_obs,dimensionality])\n",
    "X_16_Values = np.zeros([n_obs,dimensionality])\n",
    "X_20_Values = np.zeros([n_obs,dimensionality])\n",
    "X_24_Values = np.zeros([n_obs,dimensionality])\n",
    "\n",
    "for i in range(G[1].shape[0]):\n",
    "    min_eln_2  = minimize(fun=eln[0],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_3  = minimize(fun=eln[1],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_7  = minimize(fun=eln[2],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_9  = minimize(fun=eln[3],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_10  = minimize(fun=eln[4],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_13  = minimize(fun=eln[5],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_15  = minimize(fun=eln[6],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_16  = minimize(fun=eln[7],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_20  = minimize(fun=eln[8],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "    min_eln_24  = minimize(fun=eln[9],x0=linearscaletransform(np.concatenate(G, 1)[i],range_out=(-5,5)),bounds=const, method='L-BFGS-B',options=opt)\n",
    "\n",
    "    X_2_Values [i,:] = list(min_eln_2.x)\n",
    "    X_3_Values [i,:] = list(min_eln_3.x)\n",
    "    X_7_Values [i,:] = list(min_eln_7.x)\n",
    "    X_9_Values [i,:] = list(min_eln_9.x)\n",
    "    X_10_Values [i,:] = list(min_eln_10.x)                                           \n",
    "    X_13_Values [i,:] = list(min_eln_13.x)\n",
    "    X_15_Values [i,:] = list(min_eln_15.x)\n",
    "    X_16_Values [i,:] = list(min_eln_16.x) \n",
    "    X_20_Values [i,:] = list(min_eln_20.x) \n",
    "    X_24_Values [i,:] = list(min_eln_24.x)                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELN-f2\n",
      "3286200.1075317\n",
      "ELN-f3\n",
      "419.1277260951771\n",
      "ELN-f7\n",
      "1256.6571492805274\n",
      "ELN-f9\n",
      "480175.7518252373\n",
      "ELN-f10\n",
      "34305335.41833295\n",
      "ELN-f13\n",
      "1961.336694651277\n",
      "ELN-f15\n",
      "1339.569178589003\n",
      "ELN-f16\n",
      "-174.334214550833\n",
      "ELN-f20\n",
      "50637.877160544565\n",
      "ELN-f24\n",
      "374.6535104377126\n"
     ]
    }
   ],
   "source": [
    "X_2_Values = pd.DataFrame(X_2_Values)\n",
    "X_2_Values.columns = Cols\n",
    "X_2_Values.to_csv('F2_X_Values.csv')\n",
    "\n",
    "X_3_Values = pd.DataFrame(X_3_Values)\n",
    "X_3_Values.columns = Cols\n",
    "X_3_Values.to_csv('F3_X_Values.csv')\n",
    "\n",
    "X_7_Values = pd.DataFrame(X_7_Values)\n",
    "X_7_Values.columns = Cols\n",
    "X_7_Values.to_csv('F7_X_Values.csv')\n",
    "\n",
    "X_9_Values = pd.DataFrame(X_9_Values)\n",
    "X_9_Values.columns = Cols\n",
    "X_9_Values.to_csv('F9_X_Values.csv')\n",
    "\n",
    "X_10_Values = pd.DataFrame(X_10_Values)\n",
    "X_10_Values.columns = Cols\n",
    "X_10_Values.to_csv('F10_X_Values.csv')\n",
    "\n",
    "X_13_Values = pd.DataFrame(X_13_Values)\n",
    "X_13_Values.columns = Cols\n",
    "X_13_Values.to_csv('F13_X_Values.csv')\n",
    "\n",
    "X_15_Values = pd.DataFrame(X_15_Values)\n",
    "X_15_Values.columns = Cols\n",
    "X_15_Values.to_csv('F15_X_Values.csv')\n",
    "\n",
    "X_16_Values = pd.DataFrame(X_16_Values)\n",
    "X_16_Values.columns = Cols\n",
    "X_16_Values.to_csv('F16_X_Values.csv')\n",
    "\n",
    "X_20_Values = pd.DataFrame(X_20_Values)\n",
    "X_20_Values.columns = Cols\n",
    "X_20_Values.to_csv('F20_X_Values.csv')\n",
    "\n",
    "X_24_Values = pd.DataFrame(X_24_Values)\n",
    "X_24_Values.columns = Cols\n",
    "X_24_Values.to_csv('F24_X_Values.csv')\n",
    "\n",
    "\n",
    "ELN_Fun_2 = np.zeros(30)\n",
    "ELN_Fun_3 = np.zeros(30)\n",
    "ELN_Fun_7 = np.zeros(30)\n",
    "ELN_Fun_9 = np.zeros(30)\n",
    "ELN_Fun_10 = np.zeros(30)\n",
    "ELN_Fun_13 = np.zeros(30)\n",
    "ELN_Fun_15 = np.zeros(30)\n",
    "ELN_Fun_16 = np.zeros(30)\n",
    "ELN_Fun_20 = np.zeros(30)\n",
    "ELN_Fun_24 = np.zeros(30)\n",
    "for i in range(X_2_Values.shape[0]):\n",
    "    ELN_Fun_2 [i] = F2(X_2_Values.iloc[i,:])\n",
    "    ELN_Fun_3 [i] = F3(X_3_Values.iloc[i,:])\n",
    "    ELN_Fun_7 [i] = F7(X_7_Values.iloc[i,:])\n",
    "    ELN_Fun_9 [i] = F9(X_9_Values.iloc[i,:])\n",
    "    ELN_Fun_10 [i] = F10(X_10_Values.iloc[i,:])\n",
    "    ELN_Fun_13 [i] = F13(X_13_Values.iloc[i,:])\n",
    "    ELN_Fun_15 [i] = F15(X_15_Values.iloc[i,:])\n",
    "    ELN_Fun_16 [i] = F16(X_16_Values.iloc[i,:])\n",
    "    ELN_Fun_20 [i] = F20(X_20_Values.iloc[i,:])\n",
    "    ELN_Fun_24 [i] = F24(X_24_Values.iloc[i,:])\n",
    "    \n",
    "print ('ELN-f2')\n",
    "print (np.median(ELN_Fun_2) )\n",
    "print ('ELN-f3')\n",
    "print (np.median(ELN_Fun_3) )\n",
    "print ('ELN-f7')\n",
    "print (np.median(ELN_Fun_7) )\n",
    "print ('ELN-f9')\n",
    "print (np.median(ELN_Fun_9) )\n",
    "print ('ELN-f10')\n",
    "print (np.median(ELN_Fun_10) )\n",
    "print ('ELN-f13')\n",
    "print (np.median(ELN_Fun_13) )\n",
    "print ('ELN-f15')\n",
    "print (np.median(ELN_Fun_15) )\n",
    "print ('ELN-f16')\n",
    "print (np.median(ELN_Fun_16) )\n",
    "print ('ELN-f20')\n",
    "print (np.median(ELN_Fun_20) )\n",
    "print ('ELN-f24')\n",
    "print (np.median(ELN_Fun_24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.28620011e+06,  4.19127726e+02,  1.25665715e+03,  4.80175752e+05,\n",
       "        3.43053354e+07,  1.96133669e+03,  1.33956918e+03, -1.74334215e+02,\n",
       "        5.06378772e+04,  3.74653510e+02])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.median(ELN_Fun_2),np.median(ELN_Fun_3),np.median(ELN_Fun_7),np.median(ELN_Fun_9),\n",
    "np.median(ELN_Fun_10),np.median(ELN_Fun_13),np.median(ELN_Fun_15),np.median(ELN_Fun_16),\n",
    "np.median(ELN_Fun_20),np.median(ELN_Fun_24)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
