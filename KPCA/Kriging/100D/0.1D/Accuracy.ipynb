{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import sys\n",
    "import scipy.stats.distributions as dist\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dimensionality Reduction Related Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Sets & Perform Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Give the path to the training input files as the function argument, the function then \n",
    "loads the initial training and test data set\"\"\"\n",
    "def load_data(path,guidance):\n",
    "    train_data = pd.read_csv(path, index_col=0)\n",
    "    # Change all the Paths in this function depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    if (guidance == 2):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_2_200Samples.csv\"\n",
    "    elif (guidance == 3):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_3_200Samples.csv\"\n",
    "    elif (guidance == 7):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_7_200Samples.csv\"\n",
    "    elif (guidance == 9):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_9_200Samples.csv\"\n",
    "    elif (guidance == 10):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_10_200Samples.csv\"\n",
    "    elif (guidance == 13):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_13_200Samples.csv\"\n",
    "    elif (guidance == 15):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_15_200Samples.csv\"\n",
    "    elif (guidance == 16):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_16_200Samples.csv\"\n",
    "    elif (guidance == 20):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_20_200Samples.csv\"\n",
    "    elif (guidance == 24):\n",
    "        path_test = \"Data Generation/50 D/Test_Data_Sets/test_24_200Samples.csv\"\n",
    "    test_data = pd.read_csv(path_test, index_col=0)\n",
    "    data = pd.concat([ train_data, test_data ], ignore_index = True) \n",
    "    if ((data.iloc[:1000,:] == train_data).sum().sum() == train_data.shape[0] * train_data.shape[1] ):\n",
    "        if ((data.iloc[1000:,:].reset_index().iloc[:,1:] == test_data).sum().sum() == test_data.shape[0] * test_data.shape[1]):\n",
    "            print ('Successfully Merged!!!')\n",
    "            return data\n",
    "    else:\n",
    "        return 'Error'\n",
    "\n",
    "\"\"\" Performs Linear Transofrmation on the Data Set according to the paper\n",
    "Don't change anything in this function \"\"\"\n",
    "def linear_transformation(path, guidance):\n",
    "    data = load_data(path, guidance)\n",
    "    cols = data.columns[:-1]\n",
    "    # Sort all the Rows based on the value of Y = f(x) \n",
    "    # make sure that the rows with the minimum value of Y have lower weight\n",
    "    data = data.sort_values(by = ['Y'], ascending = True).reset_index()\n",
    "    # make sure tha the data set has been sorted/ranked in this order correctly\n",
    "    if (np.sum([ data ['Y'][i] < data ['Y'][i+1] for i in range(0, len(data)-1) ])  == len(data)-1):\n",
    "        print ('All Clear, Data Set Sorted in Ascending order !!!')\n",
    "        # Compute the pre weights (unnormalized) \n",
    "        pre_weights = [ np.log(len(data)) - np.log(i) for i in range(1, len(data)+1) ]\n",
    "        # Compute normalized weights \n",
    "        weights = np.diag ([ pre_weights[i] / np.sum(pre_weights) for i in range(len(pre_weights)) ])\n",
    "        X = data.iloc[:,1:-1]\n",
    "        # remove the sample mean from the data set\n",
    "        X_scaled = preprocessing.scale(X)\n",
    "        # rescale all the features based on the paper\n",
    "        X_rescaled =  pd.DataFrame(np.matmul(X_scaled.T, weights).T , columns=cols)\n",
    "        # decompose into the train and test data set again\n",
    "        X_train = X_rescaled.iloc[data.loc[data['index'] < 1000, :].index, :].reset_index().iloc[:,1:]\n",
    "        X_test = X_rescaled.iloc[data.loc[data['index'] >= 1000, :].index, :].reset_index().iloc[:,1:]\n",
    "        return X_train, X_test\n",
    "    else:\n",
    "        return 'Error'\n",
    "\n",
    "\"\"\" This is the method that implements the dimensionality reduction based on Kernel PCA \"\"\"\n",
    "def perform_dimensionality_reduction (hyper, path, guidance):\n",
    "    # Change the value in the line after this based on dimensionality reduction {0.3=30 %,0.7=70 %,0.9=90 %}\n",
    "    components = int(50-0.3 * 50) # 30 % Dimensionality Reduction\n",
    "    ker,gamm,deg,coe = hyper\n",
    "    if (ker==1):\n",
    "        ker = \"poly\"\n",
    "    elif (ker==2):\n",
    "        ker = \"rbf\"\n",
    "    elif (ker==3):\n",
    "        ker =\"sigmoid\"\n",
    "    else: # in case there is a mistake, the default shall be rbf\n",
    "        ker = \"rbf\"\n",
    "    train_data, test_data = linear_transformation(path, guidance)\n",
    "    scalar = MinMaxScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(scalar.transform (train_data)) \n",
    "    test_data = pd.DataFrame(scalar.transform (test_data))\n",
    "    kpca = KernelPCA(n_components=components,kernel=ker,gamma=np.power(10,float(gamm)),degree=deg,coef0=np.power(10,float(coe))).fit(train_data.values)\n",
    "    train_data = pd.DataFrame(kpca.transform(train_data.values))\n",
    "    test_data = pd.DataFrame(kpca.transform(test_data.values))\n",
    "    cols = []\n",
    "    for i in range(train_data.shape[1]):\n",
    "        cols.append(str('Z'+str(i+1)))\n",
    "    train_data.columns = cols\n",
    "    test_data.columns = cols\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    train_data.to_csv('KPCA_Kriging_50D_30%_latent_training_f'+path.split('/')[-1].split('_')[1][:2]+'.csv') \n",
    "    test_data.to_csv('KPCA_Kriging_50D_30%_latent_test_f'+path.split('/')[-1].split('_')[1][:2]+'.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Dimensionality Reduction Related Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New Reduced Data Sets for all Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_f2(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f2.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f2.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_2_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f3(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f3.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f3.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_3_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f7(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f7.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f7.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_7_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f9(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f9.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f9.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_9_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f10(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f10.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f10.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_10_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f13(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f13.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f13.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_13_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f15(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f15.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f15.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_15_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f16(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f16.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f16.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-43]+str('Test_Data_Sets/test_16_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f20(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f20.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f20.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_20_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true\n",
    "\n",
    "def load_f24(path):\n",
    "    # Change the Values below depending on dimensionality and % reduction\n",
    "    path_latent_train = \"KPCA_Kriging_50D_30%_latent_training_f24.csv\"\n",
    "    path_latent_test =  \"KPCA_Kriging_50D_30%_latent_test_f24.csv\"\n",
    "    train_y = pd.read_csv(path).iloc[:,-1]\n",
    "    # Change the Path Here depending upon the dimensionality {50D=200,100D=400,200D=800}\n",
    "    test_y = pd.read_csv(path[:-42]+str('Test_Data_Sets/test_24_200Samples.csv')).iloc[:,-1]\n",
    "    train = pd.read_csv(path_latent_train, index_col = 0)\n",
    "    test = pd.read_csv(path_latent_test, index_col = 0)\n",
    "    train ['Y'] = train_y\n",
    "    test ['Y'] = test_y\n",
    "    del train_y\n",
    "    del test_y\n",
    "    true = np.array(test['Y'])\n",
    "    return train,test,true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Start Kriging Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Kriging'''\n",
    "def kriging(train_data,test_data):\n",
    "    kernel =  RBF()\n",
    "    scaler = MinMaxScaler().fit(np.r_[train_data.iloc[:,:-1].values])\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel,n_restarts_optimizer= 15,random_state=0,\n",
    "                                   normalize_y=True ).fit(scaler.transform(train_data.iloc[:,:-1]), train_data.iloc[:,-1])\n",
    "    pred = gpr.predict(scaler.transform(test_data))\n",
    "    return gpr,pred\n",
    "\n",
    "\"\"\" Normalized Mean Absolute Error % \"\"\"\n",
    "def rmae(true, pred):\n",
    "    return np.mean((abs(true-pred) / abs(true)) * 100)\n",
    "\n",
    "\"\"\" This method implements and evaluates the Kriging Surrogate Model with RMAE \"\"\"\n",
    "def surrogate_model(train_data,test_data,true):\n",
    "    kri_model, kri_pred = kriging(train_data,test_data.iloc[:,:-1])\n",
    "    return rmae(true,kri_pred)\n",
    "\n",
    "\"\"\" Implements all the surrogate models, i.e., for all test function, and returns the median of RMAE errors,\n",
    "This median is used as the primary metric for Hyper-Parameters Optimization \"\"\"\n",
    "def perform_surrogate_modeling(paths):\n",
    "    train_2, test_2, true_2 = load_f2(paths[0])\n",
    "    rmae_2 = surrogate_model(train_2, test_2, true_2)\n",
    "    \n",
    "    train_3, test_3, true_3 = load_f3(paths[1])\n",
    "    rmae_3 = surrogate_model(train_3, test_3,true_3)\n",
    "    \n",
    "    train_7, test_7, true_7 = load_f7(paths[2])\n",
    "    rmae_7 = surrogate_model(train_7, test_7, true_7)\n",
    "    \n",
    "    train_9, test_9, true_9 = load_f9(paths[3])\n",
    "    rmae_9 = surrogate_model(train_9, test_9, true_9)\n",
    "    \n",
    "    train_10, test_10, true_10 = load_f10(paths[4])\n",
    "    rmae_10 = surrogate_model(train_10, test_10, true_10)\n",
    "    \n",
    "    train_13, test_13, true_13 = load_f13(paths[5])\n",
    "    rmae_13 = surrogate_model(train_13, test_13, true_13)\n",
    "    \n",
    "    train_15, test_15, true_15 = load_f15(paths[6])\n",
    "    rmae_15 = surrogate_model(train_15, test_15, true_15)\n",
    "    \n",
    "    train_16, test_16, true_16 = load_f16(paths[7])\n",
    "    rmae_16 = surrogate_model(train_16, test_16, true_16)\n",
    "    \n",
    "    train_20, test_20, true_20 = load_f20(paths[8])\n",
    "    rmae_20 = surrogate_model(train_20, test_20, true_20)\n",
    "    \n",
    "    train_24, test_24, true_24 = load_f24(paths[9])\n",
    "    rmae_24 = surrogate_model(train_24, test_24, true_24)\n",
    "    \n",
    "    accuracy = [rmae_2,rmae_3,rmae_7,rmae_9,rmae_10,rmae_13,rmae_15,rmae_16,rmae_20,rmae_24]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\"\"\" This is the function used for Hyper_Parameters_Optimization for both dimensionality reduction and surrogate modelling \"\"\"\n",
    "def hyper_parameters_optimization(hyper_dim,paths):\n",
    "    print ('Start Dimensionality Reduction:::')\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[0], 2)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[1], 3)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[2], 7)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[3], 9)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[4], 10)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[5], 13)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[6], 15)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[7], 16)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[8], 20)\n",
    "    perform_dimensionality_reduction (hyper_dim,paths[9], 24)\n",
    "    print ('End Dimensionality Reduction:::')\n",
    "    accuracy = perform_surrogate_modeling (paths)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Surrogate Modelling Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \n",
    "path_2 = \"Data Generation/50 D/Training_Data_Sets/train_2_1000Samples.csv\"\n",
    "path_3 = \"Data Generation/50 D/Training_Data_Sets/train_3_1000Samples.csv\"\n",
    "path_7 = \"Data Generation/50 D/Training_Data_Sets/train_7_1000Samples.csv\"\n",
    "path_9 = \"Data Generation/50 D/Training_Data_Sets/train_9_1000Samples.csv\"\n",
    "path_10 = \"Data Generation/50 D/Training_Data_Sets/train_10_1000Samples.csv\"\n",
    "path_13 = \"Data Generation/50 D/Training_Data_Sets/train_13_1000Samples.csv\"\n",
    "path_15 = \"Data Generation/50 D/Training_Data_Sets/train_15_1000Samples.csv\"\n",
    "path_16 = \"Data Generation/50 D/Training_Data_Sets/train_16_1000Samples.csv\"\n",
    "path_20 = \"Data Generation/50 D/Training_Data_Sets/train_201000Samples.csv\"\n",
    "path_24 = \"Data Generation/50 D/Training_Data_Sets/train_241000Samples.csv\"\n",
    "# Change the Paths here depending upon the dimensionality {50D=1000,100D=2000,200D=4000} \"\n",
    "paths = [path_2,path_3,path_7,path_9,path_10,path_13,path_15,path_16,path_20,path_24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Dimensionality Reduction:::\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "Successfully Merged!!!\n",
      "All Clear, Data Set Sorted in Ascending order !!!\n",
      "End Dimensionality Reduction:::\n",
      "The Median accuracy:::30.701902110086827\n"
     ]
    }
   ],
   "source": [
    "hyper_dim = [1,2.4768460566994888,2,0.10444378538207588]\n",
    "accuracy = hyper_parameters_optimization(hyper_dim,paths)\n",
    "print ('The Median accuracy:::'+str(np.median(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on the Possible Values of the Hyper_Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" A): Hyper-Parameters for Dimensionality Reduction::::\n",
    "\n",
    "    Kernel PCA takes four hyper parameters: Kernel,gamma, degree and coef0 respectively.\n",
    "    \n",
    "    Kernel: (Discrete Variable), Possible Values: {1,2,3}\n",
    "    gamma: float value: ranges from: {-5,5},\n",
    "    degree: (Discrete Variable), Possible Values: {2,3,4,5}\n",
    "    coef0 = float variable: {-1,1}\n",
    "\n",
    "\"\"\"\n",
    "pd.DataFrame(np.array(accuracy), columns = ['Accuracy']).to_csv('Accuracy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
